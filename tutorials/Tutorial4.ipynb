{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Jupyter Notebooks are available at https://github.com/neuro-ml/dpipe_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials on Deep Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorials introduce the library called **Deep Pipe**, which is useful for medical image analysis, including preprocessing, data augmentation, performance validation and final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 4: Putting all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current tutorial we implement our own MNIST dataset, `model_core` for classification, simple batch iterator and assemble the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nmnt/media/home/shmulev/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# import deep pipe library\n",
    "# how to install: https://github.com/neuro-ml/deep_pipe/blob/master/README.md\n",
    "import dpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data and batch iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create toy MNIST dataset for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC, ABCMeta\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dataset(ABC):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def ids(self):\n",
    "        \"\"\"Returns a tuple of ids of all objects in the dataset.\"\"\"\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Abstract class that describes a dataset for medical image classification.\"\"\"\n",
    "    @abstractmethod\n",
    "    def load_image(self, identifier: str) -> np.array:\n",
    "        \"\"\"\n",
    "        Loads a dataset entry given its identifier\n",
    "        Parameters\n",
    "        ----------\n",
    "        identifier: str\n",
    "            object's identifier\n",
    "        Returns\n",
    "        -------\n",
    "        object:\n",
    "            The entry corresponding to the identifier\n",
    "        \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_label(self, identifier: str) -> int:\n",
    "        \"\"\"\n",
    "        Loads a label for a given identifier\n",
    "        Parameters\n",
    "        ----------\n",
    "        identifier: str\n",
    "            object's identifier\n",
    "        Returns\n",
    "        -------\n",
    "        int:\n",
    "            The label corresponding to the identifier\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def n_chans_image(self) -> int:\n",
    "        \"\"\"\n",
    "        The number of channels in the input image's tensor\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        channels: int\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "class MNIST(ClassificationDataset):\n",
    "    def __init__(self, path_to_data: str):\n",
    "        # fetch mnist datasets\n",
    "        mnist = input_data.read_data_sets(path_to_data, reshape=False, one_hot=False)\n",
    "        X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "        X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "        X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "        \n",
    "        # transope the matrices so `channels` is the secord index\n",
    "        X_train = np.transpose(X_train, [0, 3, 1, 2])\n",
    "        X_validation = np.transpose(X_validation, [0, 3, 1, 2])\n",
    "        X_test = np.transpose(X_test, [0, 3, 1, 2])\n",
    "        \n",
    "        self._images = np.concatenate((X_train, X_validation, X_test), axis=0)\n",
    "        self._labels = np.concatenate((y_train, y_validation, y_test), axis=0)\n",
    "        self._n_chans_image = X_train.shape[1]\n",
    "        self._ids = list(np.arange(self._labels.shape[0]).astype(str))\n",
    "        \n",
    "        \n",
    "    def load_image(self, identifier: str) -> np.array:\n",
    "        return self._images[int(identifier), ...]\n",
    "    \n",
    "    def load_label(self, identifier: str) -> int:\n",
    "        return self._labels[int(identifier)]\n",
    "        \n",
    "    @property\n",
    "    def ids(self) -> list:\n",
    "        return self._ids\n",
    "    \n",
    "    @property\n",
    "    def n_chans_image(self) -> int:\n",
    "        return self._n_chans_image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = MNIST('MNIST_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbf39ab5b534e96b9e9516d6e76ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dpipe.medim.visualize import slice3d\n",
    "\n",
    "slice3d(mnist.load_image(mnist.ids[1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpipe.batch_iter.slices import slices\n",
    "from dpipe.train.batch_iter import make_batch_iter_from_finite\n",
    "from functools import partial\n",
    "from dpipe.medim.utils import load_by_ids\n",
    "import pdp\n",
    "\n",
    "\n",
    "def simple_iterator(ids, load_x, load_y, batch_size, *, shuffle=False):\n",
    "    def simple():\n",
    "        for x, y in load_by_ids(load_x, load_y, ids, shuffle):\n",
    "            yield x, y\n",
    "\n",
    "    return pdp.Pipeline(pdp.Source(simple(), buffer_size=5),\n",
    "                        pdp.Many2One(chunk_size=batch_size, buffer_size=2),\n",
    "                        pdp.One2One(pdp.combine_batches, buffer_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_iter = make_batch_iter_from_finite(\n",
    "    get_batch_iter=partial(\n",
    "        simple_iterator,\n",
    "        ids=mnist.ids[:16*3000],\n",
    "        load_x=mnist.load_image,\n",
    "        load_y=mnist.load_label,\n",
    "        batch_size=16,\n",
    "        shuffle=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## II. Model Core for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model core is exactly neural net which will later be used for computing logits, losses and making predictions. A model core must have the method `build`, which builds the computational graph along with placeholders and operations and returns the sequence of input placeholders and output logits. *The source: https://github.com/neuro-ml/deep_pipe/blob/develop/dpipe/model_core/base.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement LeNet-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv_block(inputs, output_channels, training, name, kernel_size=3, \n",
    "               strides=1, layer=tf.layers.conv2d, activation=tf.nn.relu):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        inputs = layer(inputs, output_channels, kernel_size=kernel_size,\n",
    "                           strides=strides, padding='same',\n",
    "                           data_format='channels_first')\n",
    "               \n",
    "        inputs = slim.batch_norm(inputs, decay=0.9, scale=True,\n",
    "                                     is_training=training,\n",
    "                                     data_format='NCHW', fused=True)\n",
    "               \n",
    "        inputs = activation(inputs)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def pooling(inputs, name, pool_size=(2,2), strides=(2,2), layer=tf.layers.max_pooling2d):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        inputs = layer(inputs, pool_size, strides, padding='same', data_format='channels_first')\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def fc(inputs, num_outputs, name, activation=tf.nn.relu):\n",
    "    with tf.name_scope(f'{name}/weights'):\n",
    "        W = weight_variable((int(inputs.shape[1]), num_outputs))\n",
    "    with tf.name_scope(f'{name}/bias'):\n",
    "        b = bias_variable([num_outputs])\n",
    "    return tf.nn.xw_plus_b(inputs, W, b, name=name)\n",
    "\n",
    "\n",
    "def build_model(inputs, classes, name, training):\n",
    "    with tf.name_scope(name):\n",
    "        inputs = conv_block(inputs, 6, training, 'layer1', kernel_size=5)\n",
    "        inputs = pooling(inputs, 'pooling1')\n",
    "\n",
    "        inputs = conv_block(inputs, 16, training, 'layer2', kernel_size=5)\n",
    "        inputs = pooling(inputs, 'pooling2')\n",
    "        inputs = tf.layers.flatten(inputs, 'flatten')\n",
    "        inputs = fc(inputs, 120, 'fc1')\n",
    "        inputs = fc(inputs, 84, 'fc2')\n",
    "        inputs = fc(inputs, classes, 'fc3', activation=tf.identity)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "def make_lenet(builder):\n",
    "    class LeNet():\n",
    "        def __init__(self, image_size, n_chans_img, classes):\n",
    "            self.image_size = image_size\n",
    "            self.n_chans_img = n_chans_img\n",
    "            self.classes = classes\n",
    "\n",
    "        def build(self, training_ph):\n",
    "            x_ph = tf.placeholder(\n",
    "                tf.float32, (None, self.n_chans_img, self.image_size, self.image_size), name='input'\n",
    "            )\n",
    "\n",
    "            logits = builder(x_ph, self.classes, 'lenet_2d', training_ph)\n",
    "            return [x_ph], logits\n",
    "\n",
    "    return LeNet\n",
    "\n",
    "\n",
    "LeNet2D = make_lenet(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lenet = LeNet2D(28, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can put all things together and finally start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpipe.tf.model import TFModel, TFFrozenModel\n",
    "from dpipe.tf.utils import get_tf_optimizer, softmax, softmax_cross_entropy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizer function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = partial(\n",
    "    get_tf_optimizer,\n",
    "    tf_optimizer_name='AdamOptimizer',\n",
    "    beta1=0.899\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a computation graph using `TFModel` class with our `model_core`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph = tf.Graph().as_default()\n",
    "tf.reset_default_graph()\n",
    "model = TFModel(lenet, softmax, softmax_cross_entropy, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import function, which wraps the training / validation process:\n",
    "\n",
    "*Source: https://github.com/neuro-ml/deep_pipe/blob/develop/dpipe/train/train.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpipe.train.train import train_base\n",
    "# here you may need to install `tensorboard_easy` package: https://pypi.python.org/pypi/tensorboard-easy/0.2.3\n",
    "# via pip: `pip install tensorboard-easy`\n",
    "\n",
    "from dpipe.train.lr_policy import Constant\n",
    "\n",
    "train_base?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `validator` function, which will be invoked each time when the epoch is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpipe.train.validator import validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def accuracy_metric(y_true, y_one_hot):\n",
    "    y_pred = np.array(y_one_hot).squeeze().argmax(axis=1)\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "val_metrics = {}\n",
    "val_metrics['accuracy'] = accuracy_metric\n",
    "\n",
    "\n",
    "def validate_fn(x, y):\n",
    "    try:\n",
    "        l = len(y)\n",
    "    except TypeError:\n",
    "        x = x[np.newaxis, ...]\n",
    "        y = np.array([y])\n",
    "    return model.do_val_step(x, y)\n",
    "  \n",
    "    \n",
    "validator = partial(\n",
    "    validate,\n",
    "    load_x=mnist.load_image,\n",
    "    load_y=mnist.load_label,\n",
    "    ids=mnist.ids[16*3000:],\n",
    "    metrics=val_metrics,\n",
    "    validate_fn=validate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `train` function for 10 epochs with constant learning rate, estimating the validation loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "lr_init = 1e-3\n",
    "train_base(model, batch_iter, n_epochs, lr_policy=Constant(lr_init), log_path='logs/', validator=validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can open `tensorboard` and check that our losses are decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
