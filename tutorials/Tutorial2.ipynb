{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Jupyter Notebooks are available at https://github.com/neuro-ml/dpipe_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials on Deep Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorials introduce the library called **Deep Pipe**, which is useful for medical image analysis, including preprocessing, data augmentation, performance validation and final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Model initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current tutorial we build a tensorflow model for image segmentation and run it on a fake data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nmnt/media/home/shmulev/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# import deep pipe library\n",
    "# how to install: https://github.com/neuro-ml/deep_pipe/blob/master/README.md\n",
    "import dpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Generate Fake Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To later test the process of model training, generate random data with random labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_image(shape):\n",
    "    return np.random.randn(*shape)[np.newaxis, ...]\n",
    "\n",
    "def generate_label(shape):\n",
    "    return np.random.randint(0, 2, size=shape)\n",
    "\n",
    "def generate_dataset(size, shape):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for _ in range(size):\n",
    "        dataset.append(generate_image(shape))\n",
    "        labels.append(generate_label(shape))\n",
    "        \n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 200\n",
    "shape = (28, 28)\n",
    "data, labels = generate_dataset(size, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAADtCAYAAABu1gaFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f376d5ab748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2*4,2*4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data[-1][0,...], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(labels[-1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### I. Model Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model core is exactly neural net which will later be used for computing logits, losses and making predictions. A model core must have the method `build`, which builds the computational graph along with placeholders and operations and returns the sequence of input placeholders and output logits. *The source: https://github.com/neuro-ml/deep_pipe/blob/develop/dpipe/model_core/base.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import UNet model:\n",
    "*(see: https://github.com/neuro-ml/deep_pipe/blob/develop/dpipe/model_core/unet.py)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpipe.model_core.unet import UNet2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize our `model_core` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2D(n_chans_in=1, n_chans_out=2, channels=[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_core` has method `build`, which builds computational graph and returns [x_ph], logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        [x_ph], logits = unet.build(tf.placeholder('bool', name='is_training'))\n",
    "        train_writer = tf.summary.FileWriter('train',\n",
    "                                      sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### II. TF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we built our computational graph (we can compute logits!), we want to compute a certain loss, create a decision function, etc. Thus, we have to wrap our `model_core` net into another object called `model`, which attaches all the needed stuff (loss, optimizer, prediction function and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dpipe.tf\n",
    "from dpipe.tf.model import TFModel, TFFrozenModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFModel** is the interface for training a neural network model.\n",
    "\n",
    "Arguments:\n",
    "- model_core: ModelCore,\n",
    "- logits2pred: callable,\n",
    "- logits2loss: callable, \n",
    "- optimize: callable\n",
    "\n",
    "**TFFrozenModel** is the interface for making inference from already trained network.\n",
    "\n",
    "Arguments:\n",
    "- model_core: ModelCore,\n",
    "- logits2pred: callable,\n",
    "- restore_model_path\n",
    "\n",
    "*For more detailts: https://github.com/neuro-ml/deep_pipe/blob/develop/dpipe/tf/model.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize the models we have to pass several function (loss, prediction, optimizer). Let's import them from **dpipe.tf.utils**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dpipe.tf.utils import softmax_cross_entropy, softmax\n",
    "from dpipe.tf.utils import get_tf_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create optimizer function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "optimize = partial(get_tf_optimizer, tf_optimizer_name='AdamOptimizer', beta1=0.899)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /nmnt/media/home/shmulev/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/training/python/training/training.py:412: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n"
     ]
    }
   ],
   "source": [
    "#reset the default graph to avoid the errors\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_model = TFModel(unet, logits2pred=softmax, logits2loss=softmax_cross_entropy, optimize=optimize)\n",
    "tf_model.session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Methods of `TFModel` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `do_train_step(*train_inputs, lr)` - makes a training step for the given data, i.e. makes forward pass, computes the loss and make an weights update.\n",
    "- `do_val_step(*val_inputs)` - makes a validation step for the given data, i.e. computes the loss and predictions. \n",
    "- `do_inf_step(*inference_inputs)` - makes a inference step for the given data, i.e. makes predictions.\n",
    "- `save(path)` - saves the current session together with the trained model.\n",
    "- `restore(path)` - restores some session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```loss = tf_model.do_train_step(*some_inputs, lr=some_lr)\n",
    "y_pred, loss = tf_model.do_val_step(*some_inputs)\n",
    "y_pred = tf_model.do_inf_step(*some_inputs)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run several train steps to track that our loss is decreasing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th step: loss = 0.746419\n",
      "0-th step: val. loss = 0.694547\n",
      "1-th step: loss = 0.744972\n",
      "2-th step: loss = 0.743591\n",
      "3-th step: loss = 0.742278\n",
      "4-th step: loss = 0.741036\n",
      "5-th step: loss = 0.739844\n",
      "6-th step: loss = 0.738689\n",
      "7-th step: loss = 0.737568\n",
      "8-th step: loss = 0.736483\n",
      "9-th step: loss = 0.735433\n",
      "10-th step: loss = 0.734408\n",
      "10-th step: val. loss = 0.699508\n",
      "11-th step: loss = 0.733412\n",
      "12-th step: loss = 0.732444\n",
      "13-th step: loss = 0.731503\n",
      "14-th step: loss = 0.730603\n",
      "15-th step: loss = 0.729727\n",
      "16-th step: loss = 0.728863\n",
      "17-th step: loss = 0.728019\n",
      "18-th step: loss = 0.727189\n",
      "19-th step: loss = 0.726369\n",
      "20-th step: loss = 0.725565\n",
      "20-th step: val. loss = 0.708092\n",
      "21-th step: loss = 0.724763\n",
      "22-th step: loss = 0.723977\n",
      "23-th step: loss = 0.723210\n",
      "24-th step: loss = 0.722456\n",
      "25-th step: loss = 0.721712\n",
      "26-th step: loss = 0.720981\n",
      "27-th step: loss = 0.720268\n",
      "28-th step: loss = 0.719573\n",
      "29-th step: loss = 0.718899\n",
      "30-th step: loss = 0.718234\n",
      "30-th step: val. loss = 0.714133\n",
      "31-th step: loss = 0.717583\n",
      "32-th step: loss = 0.716941\n",
      "33-th step: loss = 0.716305\n",
      "34-th step: loss = 0.715684\n",
      "35-th step: loss = 0.715071\n",
      "36-th step: loss = 0.714468\n",
      "37-th step: loss = 0.713868\n",
      "38-th step: loss = 0.713275\n",
      "39-th step: loss = 0.712686\n",
      "40-th step: loss = 0.712112\n",
      "40-th step: val. loss = 0.714000\n",
      "41-th step: loss = 0.711554\n",
      "42-th step: loss = 0.711013\n",
      "43-th step: loss = 0.710485\n",
      "44-th step: loss = 0.709957\n",
      "45-th step: loss = 0.709435\n",
      "46-th step: loss = 0.708928\n",
      "47-th step: loss = 0.708438\n",
      "48-th step: loss = 0.707962\n",
      "49-th step: loss = 0.707494\n",
      "50-th step: loss = 0.707036\n",
      "50-th step: val. loss = 0.712530\n",
      "51-th step: loss = 0.706583\n",
      "52-th step: loss = 0.706141\n",
      "53-th step: loss = 0.705710\n",
      "54-th step: loss = 0.705283\n",
      "55-th step: loss = 0.704864\n",
      "56-th step: loss = 0.704461\n",
      "57-th step: loss = 0.704072\n",
      "58-th step: loss = 0.703699\n",
      "59-th step: loss = 0.703334\n",
      "60-th step: loss = 0.702983\n",
      "60-th step: val. loss = 0.709754\n",
      "61-th step: loss = 0.702647\n",
      "62-th step: loss = 0.702322\n",
      "63-th step: loss = 0.702010\n",
      "64-th step: loss = 0.701709\n",
      "65-th step: loss = 0.701417\n",
      "66-th step: loss = 0.701133\n",
      "67-th step: loss = 0.700856\n",
      "68-th step: loss = 0.700590\n",
      "69-th step: loss = 0.700333\n",
      "70-th step: loss = 0.700081\n",
      "70-th step: val. loss = 0.706799\n",
      "71-th step: loss = 0.699838\n",
      "72-th step: loss = 0.699604\n",
      "73-th step: loss = 0.699381\n",
      "74-th step: loss = 0.699164\n",
      "75-th step: loss = 0.698956\n",
      "76-th step: loss = 0.698754\n",
      "77-th step: loss = 0.698560\n",
      "78-th step: loss = 0.698372\n",
      "79-th step: loss = 0.698188\n",
      "80-th step: loss = 0.698012\n",
      "80-th step: val. loss = 0.704088\n",
      "81-th step: loss = 0.697844\n",
      "82-th step: loss = 0.697682\n",
      "83-th step: loss = 0.697523\n",
      "84-th step: loss = 0.697368\n",
      "85-th step: loss = 0.697216\n",
      "86-th step: loss = 0.697068\n",
      "87-th step: loss = 0.696925\n",
      "88-th step: loss = 0.696788\n",
      "89-th step: loss = 0.696655\n",
      "90-th step: loss = 0.696526\n",
      "90-th step: val. loss = 0.702253\n",
      "91-th step: loss = 0.696400\n",
      "92-th step: loss = 0.696278\n",
      "93-th step: loss = 0.696160\n",
      "94-th step: loss = 0.696048\n",
      "95-th step: loss = 0.695942\n",
      "96-th step: loss = 0.695839\n",
      "97-th step: loss = 0.695737\n",
      "98-th step: loss = 0.695638\n",
      "99-th step: loss = 0.695543\n",
      "100-th step: loss = 0.695451\n",
      "100-th step: val. loss = 0.700981\n",
      "101-th step: loss = 0.695361\n",
      "102-th step: loss = 0.695274\n",
      "103-th step: loss = 0.695188\n",
      "104-th step: loss = 0.695104\n",
      "105-th step: loss = 0.695023\n",
      "106-th step: loss = 0.694943\n",
      "107-th step: loss = 0.694867\n",
      "108-th step: loss = 0.694792\n",
      "109-th step: loss = 0.694720\n",
      "110-th step: loss = 0.694650\n",
      "110-th step: val. loss = 0.699865\n",
      "111-th step: loss = 0.694581\n",
      "112-th step: loss = 0.694514\n",
      "113-th step: loss = 0.694449\n",
      "114-th step: loss = 0.694386\n",
      "115-th step: loss = 0.694324\n",
      "116-th step: loss = 0.694264\n",
      "117-th step: loss = 0.694204\n",
      "118-th step: loss = 0.694146\n",
      "119-th step: loss = 0.694089\n",
      "120-th step: loss = 0.694034\n",
      "120-th step: val. loss = 0.699091\n",
      "121-th step: loss = 0.693980\n",
      "122-th step: loss = 0.693928\n",
      "123-th step: loss = 0.693877\n",
      "124-th step: loss = 0.693827\n",
      "125-th step: loss = 0.693779\n",
      "126-th step: loss = 0.693732\n",
      "127-th step: loss = 0.693687\n",
      "128-th step: loss = 0.693642\n",
      "129-th step: loss = 0.693597\n",
      "130-th step: loss = 0.693553\n",
      "130-th step: val. loss = 0.698511\n",
      "131-th step: loss = 0.693510\n",
      "132-th step: loss = 0.693467\n",
      "133-th step: loss = 0.693425\n",
      "134-th step: loss = 0.693383\n",
      "135-th step: loss = 0.693342\n",
      "136-th step: loss = 0.693301\n",
      "137-th step: loss = 0.693262\n",
      "138-th step: loss = 0.693223\n",
      "139-th step: loss = 0.693184\n",
      "140-th step: loss = 0.693146\n",
      "140-th step: val. loss = 0.697977\n",
      "141-th step: loss = 0.693109\n",
      "142-th step: loss = 0.693073\n",
      "143-th step: loss = 0.693037\n",
      "144-th step: loss = 0.693002\n",
      "145-th step: loss = 0.692967\n",
      "146-th step: loss = 0.692933\n",
      "147-th step: loss = 0.692899\n",
      "148-th step: loss = 0.692866\n",
      "149-th step: loss = 0.692835\n",
      "150-th step: loss = 0.692804\n",
      "150-th step: val. loss = 0.697695\n",
      "151-th step: loss = 0.692774\n",
      "152-th step: loss = 0.692745\n",
      "153-th step: loss = 0.692716\n",
      "154-th step: loss = 0.692688\n",
      "155-th step: loss = 0.692660\n",
      "156-th step: loss = 0.692632\n",
      "157-th step: loss = 0.692604\n",
      "158-th step: loss = 0.692578\n",
      "159-th step: loss = 0.692551\n",
      "160-th step: loss = 0.692525\n",
      "160-th step: val. loss = 0.697377\n",
      "161-th step: loss = 0.692499\n",
      "162-th step: loss = 0.692474\n",
      "163-th step: loss = 0.692450\n",
      "164-th step: loss = 0.692425\n",
      "165-th step: loss = 0.692401\n",
      "166-th step: loss = 0.692377\n",
      "167-th step: loss = 0.692354\n",
      "168-th step: loss = 0.692330\n",
      "169-th step: loss = 0.692307\n",
      "170-th step: loss = 0.692284\n",
      "170-th step: val. loss = 0.697060\n",
      "171-th step: loss = 0.692261\n",
      "172-th step: loss = 0.692239\n",
      "173-th step: loss = 0.692218\n",
      "174-th step: loss = 0.692198\n",
      "175-th step: loss = 0.692177\n",
      "176-th step: loss = 0.692157\n",
      "177-th step: loss = 0.692138\n",
      "178-th step: loss = 0.692119\n",
      "179-th step: loss = 0.692100\n",
      "180-th step: loss = 0.692082\n",
      "180-th step: val. loss = 0.696816\n",
      "181-th step: loss = 0.692065\n",
      "182-th step: loss = 0.692048\n",
      "183-th step: loss = 0.692032\n",
      "184-th step: loss = 0.692016\n",
      "185-th step: loss = 0.692000\n",
      "186-th step: loss = 0.691984\n",
      "187-th step: loss = 0.691967\n",
      "188-th step: loss = 0.691951\n",
      "189-th step: loss = 0.691934\n",
      "190-th step: loss = 0.691918\n",
      "190-th step: val. loss = 0.696619\n",
      "191-th step: loss = 0.691903\n",
      "192-th step: loss = 0.691887\n",
      "193-th step: loss = 0.691871\n",
      "194-th step: loss = 0.691855\n",
      "195-th step: loss = 0.691840\n",
      "196-th step: loss = 0.691824\n",
      "197-th step: loss = 0.691809\n",
      "198-th step: loss = 0.691794\n",
      "199-th step: loss = 0.691779\n",
      "200-th step: loss = 0.691765\n",
      "200-th step: val. loss = 0.696500\n",
      "201-th step: loss = 0.691750\n",
      "202-th step: loss = 0.691735\n",
      "203-th step: loss = 0.691721\n",
      "204-th step: loss = 0.691707\n",
      "205-th step: loss = 0.691693\n",
      "206-th step: loss = 0.691679\n",
      "207-th step: loss = 0.691666\n",
      "208-th step: loss = 0.691653\n",
      "209-th step: loss = 0.691640\n",
      "210-th step: loss = 0.691626\n",
      "210-th step: val. loss = 0.696369\n",
      "211-th step: loss = 0.691614\n",
      "212-th step: loss = 0.691601\n",
      "213-th step: loss = 0.691589\n",
      "214-th step: loss = 0.691577\n",
      "215-th step: loss = 0.691565\n",
      "216-th step: loss = 0.691553\n",
      "217-th step: loss = 0.691542\n",
      "218-th step: loss = 0.691531\n",
      "219-th step: loss = 0.691520\n",
      "220-th step: loss = 0.691509\n",
      "220-th step: val. loss = 0.696327\n",
      "221-th step: loss = 0.691498\n",
      "222-th step: loss = 0.691488\n",
      "223-th step: loss = 0.691477\n",
      "224-th step: loss = 0.691467\n",
      "225-th step: loss = 0.691457\n",
      "226-th step: loss = 0.691447\n",
      "227-th step: loss = 0.691437\n",
      "228-th step: loss = 0.691428\n",
      "229-th step: loss = 0.691418\n",
      "230-th step: loss = 0.691409\n",
      "230-th step: val. loss = 0.696258\n",
      "231-th step: loss = 0.691399\n",
      "232-th step: loss = 0.691390\n",
      "233-th step: loss = 0.691381\n",
      "234-th step: loss = 0.691372\n",
      "235-th step: loss = 0.691364\n",
      "236-th step: loss = 0.691356\n",
      "237-th step: loss = 0.691348\n",
      "238-th step: loss = 0.691340\n",
      "239-th step: loss = 0.691332\n",
      "240-th step: loss = 0.691325\n",
      "240-th step: val. loss = 0.696201\n",
      "241-th step: loss = 0.691318\n",
      "242-th step: loss = 0.691311\n",
      "243-th step: loss = 0.691303\n",
      "244-th step: loss = 0.691296\n",
      "245-th step: loss = 0.691289\n",
      "246-th step: loss = 0.691282\n",
      "247-th step: loss = 0.691275\n",
      "248-th step: loss = 0.691268\n",
      "249-th step: loss = 0.691261\n",
      "250-th step: loss = 0.691254\n",
      "250-th step: val. loss = 0.696132\n",
      "251-th step: loss = 0.691248\n",
      "252-th step: loss = 0.691241\n",
      "253-th step: loss = 0.691235\n",
      "254-th step: loss = 0.691229\n",
      "255-th step: loss = 0.691222\n",
      "256-th step: loss = 0.691216\n",
      "257-th step: loss = 0.691209\n",
      "258-th step: loss = 0.691203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259-th step: loss = 0.691196\n",
      "260-th step: loss = 0.691190\n",
      "260-th step: val. loss = 0.696061\n",
      "261-th step: loss = 0.691183\n",
      "262-th step: loss = 0.691177\n",
      "263-th step: loss = 0.691171\n",
      "264-th step: loss = 0.691165\n",
      "265-th step: loss = 0.691160\n",
      "266-th step: loss = 0.691154\n",
      "267-th step: loss = 0.691148\n",
      "268-th step: loss = 0.691142\n",
      "269-th step: loss = 0.691136\n",
      "270-th step: loss = 0.691131\n",
      "270-th step: val. loss = 0.696008\n",
      "271-th step: loss = 0.691125\n",
      "272-th step: loss = 0.691120\n",
      "273-th step: loss = 0.691114\n",
      "274-th step: loss = 0.691109\n",
      "275-th step: loss = 0.691103\n",
      "276-th step: loss = 0.691098\n",
      "277-th step: loss = 0.691093\n",
      "278-th step: loss = 0.691088\n",
      "279-th step: loss = 0.691083\n",
      "280-th step: loss = 0.691078\n",
      "280-th step: val. loss = 0.695938\n",
      "281-th step: loss = 0.691073\n",
      "282-th step: loss = 0.691068\n",
      "283-th step: loss = 0.691062\n",
      "284-th step: loss = 0.691057\n",
      "285-th step: loss = 0.691052\n",
      "286-th step: loss = 0.691047\n",
      "287-th step: loss = 0.691042\n",
      "288-th step: loss = 0.691037\n",
      "289-th step: loss = 0.691032\n",
      "290-th step: loss = 0.691028\n",
      "290-th step: val. loss = 0.695890\n",
      "291-th step: loss = 0.691023\n",
      "292-th step: loss = 0.691018\n",
      "293-th step: loss = 0.691013\n",
      "294-th step: loss = 0.691008\n",
      "295-th step: loss = 0.691004\n",
      "296-th step: loss = 0.690999\n",
      "297-th step: loss = 0.690995\n",
      "298-th step: loss = 0.690990\n",
      "299-th step: loss = 0.690986\n",
      "300-th step: loss = 0.690982\n",
      "300-th step: val. loss = 0.695845\n",
      "301-th step: loss = 0.690977\n",
      "302-th step: loss = 0.690973\n",
      "303-th step: loss = 0.690969\n",
      "304-th step: loss = 0.690965\n",
      "305-th step: loss = 0.690961\n",
      "306-th step: loss = 0.690957\n",
      "307-th step: loss = 0.690953\n",
      "308-th step: loss = 0.690948\n",
      "309-th step: loss = 0.690944\n",
      "310-th step: loss = 0.690940\n",
      "310-th step: val. loss = 0.695800\n",
      "311-th step: loss = 0.690936\n",
      "312-th step: loss = 0.690932\n",
      "313-th step: loss = 0.690928\n",
      "314-th step: loss = 0.690924\n",
      "315-th step: loss = 0.690920\n",
      "316-th step: loss = 0.690916\n",
      "317-th step: loss = 0.690912\n",
      "318-th step: loss = 0.690909\n",
      "319-th step: loss = 0.690905\n",
      "320-th step: loss = 0.690901\n",
      "320-th step: val. loss = 0.695775\n",
      "321-th step: loss = 0.690897\n",
      "322-th step: loss = 0.690893\n",
      "323-th step: loss = 0.690890\n",
      "324-th step: loss = 0.690886\n",
      "325-th step: loss = 0.690882\n",
      "326-th step: loss = 0.690879\n",
      "327-th step: loss = 0.690875\n",
      "328-th step: loss = 0.690872\n",
      "329-th step: loss = 0.690868\n",
      "330-th step: loss = 0.690865\n",
      "330-th step: val. loss = 0.695768\n",
      "331-th step: loss = 0.690861\n",
      "332-th step: loss = 0.690858\n",
      "333-th step: loss = 0.690854\n",
      "334-th step: loss = 0.690851\n",
      "335-th step: loss = 0.690847\n",
      "336-th step: loss = 0.690844\n",
      "337-th step: loss = 0.690840\n",
      "338-th step: loss = 0.690837\n",
      "339-th step: loss = 0.690833\n",
      "340-th step: loss = 0.690830\n",
      "340-th step: val. loss = 0.695740\n",
      "341-th step: loss = 0.690826\n",
      "342-th step: loss = 0.690822\n",
      "343-th step: loss = 0.690818\n",
      "344-th step: loss = 0.690815\n",
      "345-th step: loss = 0.690811\n",
      "346-th step: loss = 0.690807\n",
      "347-th step: loss = 0.690803\n",
      "348-th step: loss = 0.690799\n",
      "349-th step: loss = 0.690795\n",
      "350-th step: loss = 0.690791\n",
      "350-th step: val. loss = 0.695714\n",
      "351-th step: loss = 0.690786\n",
      "352-th step: loss = 0.690781\n",
      "353-th step: loss = 0.690777\n",
      "354-th step: loss = 0.690773\n",
      "355-th step: loss = 0.690769\n",
      "356-th step: loss = 0.690764\n",
      "357-th step: loss = 0.690760\n",
      "358-th step: loss = 0.690756\n",
      "359-th step: loss = 0.690751\n",
      "360-th step: loss = 0.690747\n",
      "360-th step: val. loss = 0.695716\n",
      "361-th step: loss = 0.690743\n",
      "362-th step: loss = 0.690738\n",
      "363-th step: loss = 0.690734\n",
      "364-th step: loss = 0.690729\n",
      "365-th step: loss = 0.690725\n",
      "366-th step: loss = 0.690721\n",
      "367-th step: loss = 0.690717\n",
      "368-th step: loss = 0.690712\n",
      "369-th step: loss = 0.690708\n",
      "370-th step: loss = 0.690704\n",
      "370-th step: val. loss = 0.695731\n",
      "371-th step: loss = 0.690700\n",
      "372-th step: loss = 0.690695\n",
      "373-th step: loss = 0.690691\n",
      "374-th step: loss = 0.690687\n",
      "375-th step: loss = 0.690682\n",
      "376-th step: loss = 0.690678\n",
      "377-th step: loss = 0.690674\n",
      "378-th step: loss = 0.690669\n",
      "379-th step: loss = 0.690666\n",
      "380-th step: loss = 0.690662\n",
      "380-th step: val. loss = 0.695731\n",
      "381-th step: loss = 0.690659\n",
      "382-th step: loss = 0.690655\n",
      "383-th step: loss = 0.690651\n",
      "384-th step: loss = 0.690647\n",
      "385-th step: loss = 0.690643\n",
      "386-th step: loss = 0.690639\n",
      "387-th step: loss = 0.690635\n",
      "388-th step: loss = 0.690631\n",
      "389-th step: loss = 0.690627\n",
      "390-th step: loss = 0.690623\n",
      "390-th step: val. loss = 0.695700\n",
      "391-th step: loss = 0.690619\n",
      "392-th step: loss = 0.690615\n",
      "393-th step: loss = 0.690610\n",
      "394-th step: loss = 0.690606\n",
      "395-th step: loss = 0.690603\n",
      "396-th step: loss = 0.690598\n",
      "397-th step: loss = 0.690594\n",
      "398-th step: loss = 0.690590\n",
      "399-th step: loss = 0.690586\n",
      "400-th step: loss = 0.690582\n",
      "400-th step: val. loss = 0.695708\n",
      "401-th step: loss = 0.690578\n",
      "402-th step: loss = 0.690574\n",
      "403-th step: loss = 0.690570\n",
      "404-th step: loss = 0.690567\n",
      "405-th step: loss = 0.690563\n",
      "406-th step: loss = 0.690559\n",
      "407-th step: loss = 0.690556\n",
      "408-th step: loss = 0.690552\n",
      "409-th step: loss = 0.690548\n",
      "410-th step: loss = 0.690545\n",
      "410-th step: val. loss = 0.695726\n",
      "411-th step: loss = 0.690541\n",
      "412-th step: loss = 0.690538\n",
      "413-th step: loss = 0.690534\n",
      "414-th step: loss = 0.690531\n",
      "415-th step: loss = 0.690528\n",
      "416-th step: loss = 0.690525\n",
      "417-th step: loss = 0.690522\n",
      "418-th step: loss = 0.690518\n",
      "419-th step: loss = 0.690515\n",
      "420-th step: loss = 0.690512\n",
      "420-th step: val. loss = 0.695737\n",
      "421-th step: loss = 0.690509\n",
      "422-th step: loss = 0.690506\n",
      "423-th step: loss = 0.690503\n",
      "424-th step: loss = 0.690499\n",
      "425-th step: loss = 0.690496\n",
      "426-th step: loss = 0.690493\n",
      "427-th step: loss = 0.690490\n",
      "428-th step: loss = 0.690487\n",
      "429-th step: loss = 0.690483\n",
      "430-th step: loss = 0.690480\n",
      "430-th step: val. loss = 0.695750\n",
      "431-th step: loss = 0.690477\n",
      "432-th step: loss = 0.690474\n",
      "433-th step: loss = 0.690471\n",
      "434-th step: loss = 0.690468\n",
      "435-th step: loss = 0.690465\n",
      "436-th step: loss = 0.690461\n",
      "437-th step: loss = 0.690458\n",
      "438-th step: loss = 0.690454\n",
      "439-th step: loss = 0.690451\n",
      "440-th step: loss = 0.690447\n",
      "440-th step: val. loss = 0.695744\n",
      "441-th step: loss = 0.690444\n",
      "442-th step: loss = 0.690441\n",
      "443-th step: loss = 0.690438\n",
      "444-th step: loss = 0.690434\n",
      "445-th step: loss = 0.690432\n",
      "446-th step: loss = 0.690429\n",
      "447-th step: loss = 0.690425\n",
      "448-th step: loss = 0.690422\n",
      "449-th step: loss = 0.690419\n",
      "450-th step: loss = 0.690416\n",
      "450-th step: val. loss = 0.695746\n",
      "451-th step: loss = 0.690413\n",
      "452-th step: loss = 0.690409\n",
      "453-th step: loss = 0.690406\n",
      "454-th step: loss = 0.690403\n",
      "455-th step: loss = 0.690400\n",
      "456-th step: loss = 0.690397\n",
      "457-th step: loss = 0.690394\n",
      "458-th step: loss = 0.690391\n",
      "459-th step: loss = 0.690388\n",
      "460-th step: loss = 0.690385\n",
      "460-th step: val. loss = 0.695740\n",
      "461-th step: loss = 0.690382\n",
      "462-th step: loss = 0.690379\n",
      "463-th step: loss = 0.690376\n",
      "464-th step: loss = 0.690373\n",
      "465-th step: loss = 0.690370\n",
      "466-th step: loss = 0.690367\n",
      "467-th step: loss = 0.690365\n",
      "468-th step: loss = 0.690362\n",
      "469-th step: loss = 0.690359\n",
      "470-th step: loss = 0.690356\n",
      "470-th step: val. loss = 0.695778\n",
      "471-th step: loss = 0.690353\n",
      "472-th step: loss = 0.690350\n",
      "473-th step: loss = 0.690347\n",
      "474-th step: loss = 0.690345\n",
      "475-th step: loss = 0.690341\n",
      "476-th step: loss = 0.690338\n",
      "477-th step: loss = 0.690336\n",
      "478-th step: loss = 0.690333\n",
      "479-th step: loss = 0.690330\n",
      "480-th step: loss = 0.690327\n",
      "480-th step: val. loss = 0.695779\n",
      "481-th step: loss = 0.690324\n",
      "482-th step: loss = 0.690321\n",
      "483-th step: loss = 0.690318\n",
      "484-th step: loss = 0.690316\n",
      "485-th step: loss = 0.690313\n",
      "486-th step: loss = 0.690311\n",
      "487-th step: loss = 0.690308\n",
      "488-th step: loss = 0.690306\n",
      "489-th step: loss = 0.690303\n",
      "490-th step: loss = 0.690300\n",
      "490-th step: val. loss = 0.695795\n",
      "491-th step: loss = 0.690298\n",
      "492-th step: loss = 0.690295\n",
      "493-th step: loss = 0.690293\n",
      "494-th step: loss = 0.690290\n",
      "495-th step: loss = 0.690288\n",
      "496-th step: loss = 0.690285\n",
      "497-th step: loss = 0.690282\n",
      "498-th step: loss = 0.690280\n",
      "499-th step: loss = 0.690277\n",
      "500-th step: loss = 0.690275\n",
      "500-th step: val. loss = 0.695793\n",
      "501-th step: loss = 0.690273\n",
      "502-th step: loss = 0.690270\n",
      "503-th step: loss = 0.690267\n",
      "504-th step: loss = 0.690265\n",
      "505-th step: loss = 0.690262\n",
      "506-th step: loss = 0.690260\n",
      "507-th step: loss = 0.690257\n",
      "508-th step: loss = 0.690254\n",
      "509-th step: loss = 0.690252\n",
      "510-th step: loss = 0.690249\n",
      "510-th step: val. loss = 0.695784\n",
      "511-th step: loss = 0.690246\n",
      "512-th step: loss = 0.690243\n",
      "513-th step: loss = 0.690240\n",
      "514-th step: loss = 0.690237\n",
      "515-th step: loss = 0.690234\n",
      "516-th step: loss = 0.690231\n",
      "517-th step: loss = 0.690228\n",
      "518-th step: loss = 0.690225\n",
      "519-th step: loss = 0.690221\n",
      "520-th step: loss = 0.690218\n",
      "520-th step: val. loss = 0.695775\n",
      "521-th step: loss = 0.690215\n",
      "522-th step: loss = 0.690212\n",
      "523-th step: loss = 0.690209\n",
      "524-th step: loss = 0.690206\n",
      "525-th step: loss = 0.690202\n",
      "526-th step: loss = 0.690199\n",
      "527-th step: loss = 0.690196\n",
      "528-th step: loss = 0.690192\n",
      "529-th step: loss = 0.690188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530-th step: loss = 0.690185\n",
      "530-th step: val. loss = 0.695797\n",
      "531-th step: loss = 0.690181\n",
      "532-th step: loss = 0.690178\n",
      "533-th step: loss = 0.690174\n",
      "534-th step: loss = 0.690171\n",
      "535-th step: loss = 0.690167\n",
      "536-th step: loss = 0.690164\n",
      "537-th step: loss = 0.690161\n",
      "538-th step: loss = 0.690157\n",
      "539-th step: loss = 0.690154\n",
      "540-th step: loss = 0.690151\n",
      "540-th step: val. loss = 0.695809\n",
      "541-th step: loss = 0.690148\n",
      "542-th step: loss = 0.690145\n",
      "543-th step: loss = 0.690142\n",
      "544-th step: loss = 0.690139\n",
      "545-th step: loss = 0.690136\n",
      "546-th step: loss = 0.690133\n",
      "547-th step: loss = 0.690130\n",
      "548-th step: loss = 0.690127\n",
      "549-th step: loss = 0.690125\n",
      "550-th step: loss = 0.690122\n",
      "550-th step: val. loss = 0.695819\n",
      "551-th step: loss = 0.690120\n",
      "552-th step: loss = 0.690117\n",
      "553-th step: loss = 0.690114\n",
      "554-th step: loss = 0.690111\n",
      "555-th step: loss = 0.690108\n",
      "556-th step: loss = 0.690105\n",
      "557-th step: loss = 0.690102\n",
      "558-th step: loss = 0.690100\n",
      "559-th step: loss = 0.690097\n",
      "560-th step: loss = 0.690094\n",
      "560-th step: val. loss = 0.695829\n",
      "561-th step: loss = 0.690091\n",
      "562-th step: loss = 0.690088\n",
      "563-th step: loss = 0.690085\n",
      "564-th step: loss = 0.690082\n",
      "565-th step: loss = 0.690078\n",
      "566-th step: loss = 0.690075\n",
      "567-th step: loss = 0.690072\n",
      "568-th step: loss = 0.690069\n",
      "569-th step: loss = 0.690066\n",
      "570-th step: loss = 0.690062\n",
      "570-th step: val. loss = 0.695813\n",
      "571-th step: loss = 0.690059\n",
      "572-th step: loss = 0.690056\n",
      "573-th step: loss = 0.690053\n",
      "574-th step: loss = 0.690050\n",
      "575-th step: loss = 0.690048\n",
      "576-th step: loss = 0.690045\n",
      "577-th step: loss = 0.690042\n",
      "578-th step: loss = 0.690040\n",
      "579-th step: loss = 0.690037\n",
      "580-th step: loss = 0.690035\n",
      "580-th step: val. loss = 0.695834\n",
      "581-th step: loss = 0.690033\n",
      "582-th step: loss = 0.690030\n",
      "583-th step: loss = 0.690028\n",
      "584-th step: loss = 0.690025\n",
      "585-th step: loss = 0.690022\n",
      "586-th step: loss = 0.690019\n",
      "587-th step: loss = 0.690017\n",
      "588-th step: loss = 0.690014\n",
      "589-th step: loss = 0.690011\n",
      "590-th step: loss = 0.690009\n",
      "590-th step: val. loss = 0.695822\n",
      "591-th step: loss = 0.690006\n",
      "592-th step: loss = 0.690003\n",
      "593-th step: loss = 0.690000\n",
      "594-th step: loss = 0.689997\n",
      "595-th step: loss = 0.689995\n",
      "596-th step: loss = 0.689992\n",
      "597-th step: loss = 0.689989\n",
      "598-th step: loss = 0.689986\n",
      "599-th step: loss = 0.689983\n",
      "600-th step: loss = 0.689980\n",
      "600-th step: val. loss = 0.695831\n",
      "601-th step: loss = 0.689977\n",
      "602-th step: loss = 0.689974\n",
      "603-th step: loss = 0.689971\n",
      "604-th step: loss = 0.689968\n",
      "605-th step: loss = 0.689966\n",
      "606-th step: loss = 0.689963\n",
      "607-th step: loss = 0.689960\n",
      "608-th step: loss = 0.689957\n",
      "609-th step: loss = 0.689954\n",
      "610-th step: loss = 0.689952\n",
      "610-th step: val. loss = 0.695867\n",
      "611-th step: loss = 0.689949\n",
      "612-th step: loss = 0.689947\n",
      "613-th step: loss = 0.689945\n",
      "614-th step: loss = 0.689942\n",
      "615-th step: loss = 0.689939\n",
      "616-th step: loss = 0.689937\n",
      "617-th step: loss = 0.689934\n",
      "618-th step: loss = 0.689932\n",
      "619-th step: loss = 0.689929\n",
      "620-th step: loss = 0.689926\n",
      "620-th step: val. loss = 0.695908\n",
      "621-th step: loss = 0.689924\n",
      "622-th step: loss = 0.689921\n",
      "623-th step: loss = 0.689919\n",
      "624-th step: loss = 0.689917\n",
      "625-th step: loss = 0.689914\n",
      "626-th step: loss = 0.689912\n",
      "627-th step: loss = 0.689910\n",
      "628-th step: loss = 0.689908\n",
      "629-th step: loss = 0.689906\n",
      "630-th step: loss = 0.689903\n",
      "630-th step: val. loss = 0.695921\n",
      "631-th step: loss = 0.689901\n",
      "632-th step: loss = 0.689898\n",
      "633-th step: loss = 0.689895\n",
      "634-th step: loss = 0.689893\n",
      "635-th step: loss = 0.689891\n",
      "636-th step: loss = 0.689888\n",
      "637-th step: loss = 0.689885\n",
      "638-th step: loss = 0.689883\n",
      "639-th step: loss = 0.689881\n",
      "640-th step: loss = 0.689878\n",
      "640-th step: val. loss = 0.695944\n",
      "641-th step: loss = 0.689876\n",
      "642-th step: loss = 0.689873\n",
      "643-th step: loss = 0.689871\n",
      "644-th step: loss = 0.689868\n",
      "645-th step: loss = 0.689866\n",
      "646-th step: loss = 0.689863\n",
      "647-th step: loss = 0.689861\n",
      "648-th step: loss = 0.689859\n",
      "649-th step: loss = 0.689856\n",
      "650-th step: loss = 0.689854\n",
      "650-th step: val. loss = 0.695970\n",
      "651-th step: loss = 0.689852\n",
      "652-th step: loss = 0.689850\n",
      "653-th step: loss = 0.689847\n",
      "654-th step: loss = 0.689845\n",
      "655-th step: loss = 0.689842\n",
      "656-th step: loss = 0.689840\n",
      "657-th step: loss = 0.689837\n",
      "658-th step: loss = 0.689834\n",
      "659-th step: loss = 0.689832\n",
      "660-th step: loss = 0.689828\n",
      "660-th step: val. loss = 0.696004\n",
      "661-th step: loss = 0.689825\n",
      "662-th step: loss = 0.689822\n",
      "663-th step: loss = 0.689820\n",
      "664-th step: loss = 0.689817\n",
      "665-th step: loss = 0.689814\n",
      "666-th step: loss = 0.689811\n",
      "667-th step: loss = 0.689808\n",
      "668-th step: loss = 0.689805\n",
      "669-th step: loss = 0.689802\n",
      "670-th step: loss = 0.689799\n",
      "670-th step: val. loss = 0.696014\n",
      "671-th step: loss = 0.689796\n",
      "672-th step: loss = 0.689793\n",
      "673-th step: loss = 0.689790\n",
      "674-th step: loss = 0.689786\n",
      "675-th step: loss = 0.689783\n",
      "676-th step: loss = 0.689780\n",
      "677-th step: loss = 0.689777\n",
      "678-th step: loss = 0.689775\n",
      "679-th step: loss = 0.689772\n",
      "680-th step: loss = 0.689769\n",
      "680-th step: val. loss = 0.696046\n",
      "681-th step: loss = 0.689766\n",
      "682-th step: loss = 0.689763\n",
      "683-th step: loss = 0.689760\n",
      "684-th step: loss = 0.689757\n",
      "685-th step: loss = 0.689754\n",
      "686-th step: loss = 0.689751\n",
      "687-th step: loss = 0.689747\n",
      "688-th step: loss = 0.689744\n",
      "689-th step: loss = 0.689741\n",
      "690-th step: loss = 0.689738\n",
      "690-th step: val. loss = 0.696055\n",
      "691-th step: loss = 0.689734\n",
      "692-th step: loss = 0.689731\n",
      "693-th step: loss = 0.689728\n",
      "694-th step: loss = 0.689725\n",
      "695-th step: loss = 0.689722\n",
      "696-th step: loss = 0.689719\n",
      "697-th step: loss = 0.689716\n",
      "698-th step: loss = 0.689713\n",
      "699-th step: loss = 0.689710\n",
      "700-th step: loss = 0.689708\n",
      "700-th step: val. loss = 0.696081\n",
      "701-th step: loss = 0.689705\n",
      "702-th step: loss = 0.689702\n",
      "703-th step: loss = 0.689700\n",
      "704-th step: loss = 0.689697\n",
      "705-th step: loss = 0.689694\n",
      "706-th step: loss = 0.689692\n",
      "707-th step: loss = 0.689689\n",
      "708-th step: loss = 0.689686\n",
      "709-th step: loss = 0.689683\n",
      "710-th step: loss = 0.689681\n",
      "710-th step: val. loss = 0.696119\n",
      "711-th step: loss = 0.689678\n",
      "712-th step: loss = 0.689675\n",
      "713-th step: loss = 0.689672\n",
      "714-th step: loss = 0.689669\n",
      "715-th step: loss = 0.689667\n",
      "716-th step: loss = 0.689664\n",
      "717-th step: loss = 0.689661\n",
      "718-th step: loss = 0.689658\n",
      "719-th step: loss = 0.689655\n",
      "720-th step: loss = 0.689652\n",
      "720-th step: val. loss = 0.696144\n",
      "721-th step: loss = 0.689650\n",
      "722-th step: loss = 0.689647\n",
      "723-th step: loss = 0.689644\n",
      "724-th step: loss = 0.689641\n",
      "725-th step: loss = 0.689638\n",
      "726-th step: loss = 0.689635\n",
      "727-th step: loss = 0.689633\n",
      "728-th step: loss = 0.689630\n",
      "729-th step: loss = 0.689627\n",
      "730-th step: loss = 0.689624\n",
      "730-th step: val. loss = 0.696189\n",
      "731-th step: loss = 0.689621\n",
      "732-th step: loss = 0.689617\n",
      "733-th step: loss = 0.689614\n",
      "734-th step: loss = 0.689611\n",
      "735-th step: loss = 0.689608\n",
      "736-th step: loss = 0.689606\n",
      "737-th step: loss = 0.689603\n",
      "738-th step: loss = 0.689600\n",
      "739-th step: loss = 0.689598\n",
      "740-th step: loss = 0.689595\n",
      "740-th step: val. loss = 0.696248\n",
      "741-th step: loss = 0.689592\n",
      "742-th step: loss = 0.689590\n",
      "743-th step: loss = 0.689587\n",
      "744-th step: loss = 0.689584\n",
      "745-th step: loss = 0.689581\n",
      "746-th step: loss = 0.689578\n",
      "747-th step: loss = 0.689575\n",
      "748-th step: loss = 0.689573\n",
      "749-th step: loss = 0.689570\n",
      "750-th step: loss = 0.689568\n",
      "750-th step: val. loss = 0.696262\n",
      "751-th step: loss = 0.689565\n",
      "752-th step: loss = 0.689563\n",
      "753-th step: loss = 0.689561\n",
      "754-th step: loss = 0.689558\n",
      "755-th step: loss = 0.689556\n",
      "756-th step: loss = 0.689553\n",
      "757-th step: loss = 0.689551\n",
      "758-th step: loss = 0.689549\n",
      "759-th step: loss = 0.689547\n",
      "760-th step: loss = 0.689544\n",
      "760-th step: val. loss = 0.696293\n",
      "761-th step: loss = 0.689542\n",
      "762-th step: loss = 0.689539\n",
      "763-th step: loss = 0.689537\n",
      "764-th step: loss = 0.689534\n",
      "765-th step: loss = 0.689532\n",
      "766-th step: loss = 0.689530\n",
      "767-th step: loss = 0.689527\n",
      "768-th step: loss = 0.689525\n",
      "769-th step: loss = 0.689523\n",
      "770-th step: loss = 0.689521\n",
      "770-th step: val. loss = 0.696306\n",
      "771-th step: loss = 0.689518\n",
      "772-th step: loss = 0.689516\n",
      "773-th step: loss = 0.689514\n",
      "774-th step: loss = 0.689512\n",
      "775-th step: loss = 0.689510\n",
      "776-th step: loss = 0.689507\n",
      "777-th step: loss = 0.689505\n",
      "778-th step: loss = 0.689502\n",
      "779-th step: loss = 0.689500\n",
      "780-th step: loss = 0.689498\n",
      "780-th step: val. loss = 0.696331\n",
      "781-th step: loss = 0.689496\n",
      "782-th step: loss = 0.689495\n",
      "783-th step: loss = 0.689493\n",
      "784-th step: loss = 0.689490\n",
      "785-th step: loss = 0.689488\n",
      "786-th step: loss = 0.689486\n",
      "787-th step: loss = 0.689484\n",
      "788-th step: loss = 0.689482\n",
      "789-th step: loss = 0.689481\n",
      "790-th step: loss = 0.689478\n",
      "790-th step: val. loss = 0.696365\n",
      "791-th step: loss = 0.689476\n",
      "792-th step: loss = 0.689474\n",
      "793-th step: loss = 0.689472\n",
      "794-th step: loss = 0.689471\n",
      "795-th step: loss = 0.689469\n",
      "796-th step: loss = 0.689466\n",
      "797-th step: loss = 0.689465\n",
      "798-th step: loss = 0.689463\n",
      "799-th step: loss = 0.689461\n",
      "800-th step: loss = 0.689459\n",
      "800-th step: val. loss = 0.696400\n",
      "801-th step: loss = 0.689457\n",
      "802-th step: loss = 0.689455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803-th step: loss = 0.689453\n",
      "804-th step: loss = 0.689451\n",
      "805-th step: loss = 0.689449\n",
      "806-th step: loss = 0.689447\n",
      "807-th step: loss = 0.689446\n",
      "808-th step: loss = 0.689444\n",
      "809-th step: loss = 0.689442\n",
      "810-th step: loss = 0.689440\n",
      "810-th step: val. loss = 0.696422\n",
      "811-th step: loss = 0.689438\n",
      "812-th step: loss = 0.689436\n",
      "813-th step: loss = 0.689434\n",
      "814-th step: loss = 0.689432\n",
      "815-th step: loss = 0.689430\n",
      "816-th step: loss = 0.689428\n",
      "817-th step: loss = 0.689426\n",
      "818-th step: loss = 0.689424\n",
      "819-th step: loss = 0.689422\n",
      "820-th step: loss = 0.689419\n",
      "820-th step: val. loss = 0.696445\n",
      "821-th step: loss = 0.689417\n",
      "822-th step: loss = 0.689415\n",
      "823-th step: loss = 0.689412\n",
      "824-th step: loss = 0.689410\n",
      "825-th step: loss = 0.689408\n",
      "826-th step: loss = 0.689405\n",
      "827-th step: loss = 0.689403\n",
      "828-th step: loss = 0.689401\n",
      "829-th step: loss = 0.689398\n",
      "830-th step: loss = 0.689396\n",
      "830-th step: val. loss = 0.696466\n",
      "831-th step: loss = 0.689393\n",
      "832-th step: loss = 0.689391\n",
      "833-th step: loss = 0.689388\n",
      "834-th step: loss = 0.689386\n",
      "835-th step: loss = 0.689384\n",
      "836-th step: loss = 0.689381\n",
      "837-th step: loss = 0.689379\n",
      "838-th step: loss = 0.689377\n",
      "839-th step: loss = 0.689374\n",
      "840-th step: loss = 0.689372\n",
      "840-th step: val. loss = 0.696476\n",
      "841-th step: loss = 0.689370\n",
      "842-th step: loss = 0.689367\n",
      "843-th step: loss = 0.689365\n",
      "844-th step: loss = 0.689363\n",
      "845-th step: loss = 0.689361\n",
      "846-th step: loss = 0.689359\n",
      "847-th step: loss = 0.689357\n",
      "848-th step: loss = 0.689354\n",
      "849-th step: loss = 0.689352\n",
      "850-th step: loss = 0.689350\n",
      "850-th step: val. loss = 0.696477\n",
      "851-th step: loss = 0.689348\n",
      "852-th step: loss = 0.689346\n",
      "853-th step: loss = 0.689344\n",
      "854-th step: loss = 0.689341\n",
      "855-th step: loss = 0.689339\n",
      "856-th step: loss = 0.689337\n",
      "857-th step: loss = 0.689335\n",
      "858-th step: loss = 0.689333\n",
      "859-th step: loss = 0.689331\n",
      "860-th step: loss = 0.689329\n",
      "860-th step: val. loss = 0.696480\n",
      "861-th step: loss = 0.689327\n",
      "862-th step: loss = 0.689326\n",
      "863-th step: loss = 0.689324\n",
      "864-th step: loss = 0.689322\n",
      "865-th step: loss = 0.689320\n",
      "866-th step: loss = 0.689318\n",
      "867-th step: loss = 0.689315\n",
      "868-th step: loss = 0.689314\n",
      "869-th step: loss = 0.689312\n",
      "870-th step: loss = 0.689310\n",
      "870-th step: val. loss = 0.696502\n",
      "871-th step: loss = 0.689308\n",
      "872-th step: loss = 0.689306\n",
      "873-th step: loss = 0.689304\n",
      "874-th step: loss = 0.689302\n",
      "875-th step: loss = 0.689300\n",
      "876-th step: loss = 0.689298\n",
      "877-th step: loss = 0.689296\n",
      "878-th step: loss = 0.689294\n",
      "879-th step: loss = 0.689292\n",
      "880-th step: loss = 0.689290\n",
      "880-th step: val. loss = 0.696515\n",
      "881-th step: loss = 0.689288\n",
      "882-th step: loss = 0.689286\n",
      "883-th step: loss = 0.689284\n",
      "884-th step: loss = 0.689281\n",
      "885-th step: loss = 0.689279\n",
      "886-th step: loss = 0.689277\n",
      "887-th step: loss = 0.689274\n",
      "888-th step: loss = 0.689272\n",
      "889-th step: loss = 0.689269\n",
      "890-th step: loss = 0.689266\n",
      "890-th step: val. loss = 0.696503\n",
      "891-th step: loss = 0.689263\n",
      "892-th step: loss = 0.689259\n",
      "893-th step: loss = 0.689257\n",
      "894-th step: loss = 0.689254\n",
      "895-th step: loss = 0.689252\n",
      "896-th step: loss = 0.689249\n",
      "897-th step: loss = 0.689247\n",
      "898-th step: loss = 0.689244\n",
      "899-th step: loss = 0.689242\n",
      "900-th step: loss = 0.689240\n",
      "900-th step: val. loss = 0.696509\n",
      "901-th step: loss = 0.689237\n",
      "902-th step: loss = 0.689235\n",
      "903-th step: loss = 0.689233\n",
      "904-th step: loss = 0.689230\n",
      "905-th step: loss = 0.689229\n",
      "906-th step: loss = 0.689226\n",
      "907-th step: loss = 0.689224\n",
      "908-th step: loss = 0.689222\n",
      "909-th step: loss = 0.689220\n",
      "910-th step: loss = 0.689218\n",
      "910-th step: val. loss = 0.696545\n",
      "911-th step: loss = 0.689217\n",
      "912-th step: loss = 0.689215\n",
      "913-th step: loss = 0.689213\n",
      "914-th step: loss = 0.689211\n",
      "915-th step: loss = 0.689209\n",
      "916-th step: loss = 0.689207\n",
      "917-th step: loss = 0.689205\n",
      "918-th step: loss = 0.689203\n",
      "919-th step: loss = 0.689201\n",
      "920-th step: loss = 0.689199\n",
      "920-th step: val. loss = 0.696544\n",
      "921-th step: loss = 0.689197\n",
      "922-th step: loss = 0.689195\n",
      "923-th step: loss = 0.689193\n",
      "924-th step: loss = 0.689191\n",
      "925-th step: loss = 0.689189\n",
      "926-th step: loss = 0.689187\n",
      "927-th step: loss = 0.689186\n",
      "928-th step: loss = 0.689184\n",
      "929-th step: loss = 0.689182\n",
      "930-th step: loss = 0.689181\n",
      "930-th step: val. loss = 0.696561\n",
      "931-th step: loss = 0.689179\n",
      "932-th step: loss = 0.689177\n",
      "933-th step: loss = 0.689176\n",
      "934-th step: loss = 0.689174\n",
      "935-th step: loss = 0.689172\n",
      "936-th step: loss = 0.689171\n",
      "937-th step: loss = 0.689169\n",
      "938-th step: loss = 0.689168\n",
      "939-th step: loss = 0.689166\n",
      "940-th step: loss = 0.689165\n",
      "940-th step: val. loss = 0.696595\n",
      "941-th step: loss = 0.689163\n",
      "942-th step: loss = 0.689162\n",
      "943-th step: loss = 0.689160\n",
      "944-th step: loss = 0.689158\n",
      "945-th step: loss = 0.689157\n",
      "946-th step: loss = 0.689155\n",
      "947-th step: loss = 0.689153\n",
      "948-th step: loss = 0.689152\n",
      "949-th step: loss = 0.689150\n",
      "950-th step: loss = 0.689148\n",
      "950-th step: val. loss = 0.696603\n",
      "951-th step: loss = 0.689147\n",
      "952-th step: loss = 0.689145\n",
      "953-th step: loss = 0.689143\n",
      "954-th step: loss = 0.689142\n",
      "955-th step: loss = 0.689140\n",
      "956-th step: loss = 0.689139\n",
      "957-th step: loss = 0.689137\n",
      "958-th step: loss = 0.689135\n",
      "959-th step: loss = 0.689134\n",
      "960-th step: loss = 0.689132\n",
      "960-th step: val. loss = 0.696614\n",
      "961-th step: loss = 0.689131\n",
      "962-th step: loss = 0.689129\n",
      "963-th step: loss = 0.689128\n",
      "964-th step: loss = 0.689126\n",
      "965-th step: loss = 0.689125\n",
      "966-th step: loss = 0.689123\n",
      "967-th step: loss = 0.689122\n",
      "968-th step: loss = 0.689120\n",
      "969-th step: loss = 0.689119\n",
      "970-th step: loss = 0.689117\n",
      "970-th step: val. loss = 0.696657\n",
      "971-th step: loss = 0.689116\n",
      "972-th step: loss = 0.689114\n",
      "973-th step: loss = 0.689112\n",
      "974-th step: loss = 0.689111\n",
      "975-th step: loss = 0.689109\n",
      "976-th step: loss = 0.689108\n",
      "977-th step: loss = 0.689106\n",
      "978-th step: loss = 0.689105\n",
      "979-th step: loss = 0.689103\n",
      "980-th step: loss = 0.689101\n",
      "980-th step: val. loss = 0.696669\n",
      "981-th step: loss = 0.689100\n",
      "982-th step: loss = 0.689098\n",
      "983-th step: loss = 0.689097\n",
      "984-th step: loss = 0.689096\n",
      "985-th step: loss = 0.689094\n",
      "986-th step: loss = 0.689093\n",
      "987-th step: loss = 0.689091\n",
      "988-th step: loss = 0.689090\n",
      "989-th step: loss = 0.689088\n",
      "990-th step: loss = 0.689086\n",
      "990-th step: val. loss = 0.696688\n",
      "991-th step: loss = 0.689085\n",
      "992-th step: loss = 0.689083\n",
      "993-th step: loss = 0.689081\n",
      "994-th step: loss = 0.689080\n",
      "995-th step: loss = 0.689078\n",
      "996-th step: loss = 0.689077\n",
      "997-th step: loss = 0.689075\n",
      "998-th step: loss = 0.689074\n",
      "999-th step: loss = 0.689072\n"
     ]
    }
   ],
   "source": [
    "n_steps = 1000\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "#reset the default graph to avoid the errors\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_model = TFModel(unet, logits2pred=softmax, logits2loss=softmax_cross_entropy, optimize=optimize)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    loss = tf_model.do_train_step(data[:100], labels[:100], lr=1e-3)\n",
    "    train_loss.append(loss)\n",
    "    print('%d-th step: loss = %f' % (i, loss))\n",
    "    if i % 10 == 0:\n",
    "        y_pred, loss = tf_model.do_val_step(data[100:], labels[100:])\n",
    "        val_loss.extend([loss]*10)\n",
    "        print('%d-th step: val. loss = %f' % (i, loss))\n",
    "# after the training is done\n",
    "tf_model.save('train')\n",
    "tf_model.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAE/CAYAAAA6+mr5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3700623f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(train_loss, 'r',label='train')\n",
    "plt.plot(val_loss, 'b', label='validation')\n",
    "plt.title('Loss during training')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is done, we saved the model to `~/train` directory. We can restore that session and continue the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/model\n",
      "0-th step: loss = 0.689070\n",
      "0-th step: val. loss = 0.696712\n",
      "1-th step: loss = 0.689068\n",
      "2-th step: loss = 0.689067\n",
      "3-th step: loss = 0.689065\n",
      "4-th step: loss = 0.689064\n",
      "5-th step: loss = 0.689062\n",
      "6-th step: loss = 0.689060\n",
      "7-th step: loss = 0.689059\n",
      "8-th step: loss = 0.689057\n",
      "9-th step: loss = 0.689055\n",
      "10-th step: loss = 0.689053\n",
      "10-th step: val. loss = 0.696736\n",
      "11-th step: loss = 0.689051\n",
      "12-th step: loss = 0.689050\n",
      "13-th step: loss = 0.689048\n",
      "14-th step: loss = 0.689046\n",
      "15-th step: loss = 0.689044\n",
      "16-th step: loss = 0.689042\n",
      "17-th step: loss = 0.689040\n",
      "18-th step: loss = 0.689037\n",
      "19-th step: loss = 0.689035\n",
      "20-th step: loss = 0.689033\n",
      "20-th step: val. loss = 0.696730\n",
      "21-th step: loss = 0.689031\n",
      "22-th step: loss = 0.689028\n",
      "23-th step: loss = 0.689026\n",
      "24-th step: loss = 0.689024\n",
      "25-th step: loss = 0.689021\n",
      "26-th step: loss = 0.689019\n",
      "27-th step: loss = 0.689017\n",
      "28-th step: loss = 0.689016\n",
      "29-th step: loss = 0.689014\n",
      "30-th step: loss = 0.689012\n",
      "30-th step: val. loss = 0.696746\n",
      "31-th step: loss = 0.689010\n",
      "32-th step: loss = 0.689008\n",
      "33-th step: loss = 0.689006\n",
      "34-th step: loss = 0.689004\n",
      "35-th step: loss = 0.689002\n",
      "36-th step: loss = 0.689000\n",
      "37-th step: loss = 0.688998\n",
      "38-th step: loss = 0.688995\n",
      "39-th step: loss = 0.688993\n",
      "40-th step: loss = 0.688991\n",
      "40-th step: val. loss = 0.696777\n",
      "41-th step: loss = 0.688990\n",
      "42-th step: loss = 0.688988\n",
      "43-th step: loss = 0.688986\n",
      "44-th step: loss = 0.688984\n",
      "45-th step: loss = 0.688982\n",
      "46-th step: loss = 0.688979\n",
      "47-th step: loss = 0.688977\n",
      "48-th step: loss = 0.688975\n",
      "49-th step: loss = 0.688973\n",
      "50-th step: loss = 0.688971\n",
      "50-th step: val. loss = 0.696820\n",
      "51-th step: loss = 0.688969\n",
      "52-th step: loss = 0.688966\n",
      "53-th step: loss = 0.688964\n",
      "54-th step: loss = 0.688962\n",
      "55-th step: loss = 0.688959\n",
      "56-th step: loss = 0.688957\n",
      "57-th step: loss = 0.688955\n",
      "58-th step: loss = 0.688953\n",
      "59-th step: loss = 0.688951\n",
      "60-th step: loss = 0.688949\n",
      "60-th step: val. loss = 0.696828\n",
      "61-th step: loss = 0.688947\n",
      "62-th step: loss = 0.688945\n",
      "63-th step: loss = 0.688943\n",
      "64-th step: loss = 0.688941\n",
      "65-th step: loss = 0.688939\n",
      "66-th step: loss = 0.688937\n",
      "67-th step: loss = 0.688935\n",
      "68-th step: loss = 0.688932\n",
      "69-th step: loss = 0.688930\n",
      "70-th step: loss = 0.688927\n",
      "70-th step: val. loss = 0.696894\n",
      "71-th step: loss = 0.688924\n",
      "72-th step: loss = 0.688922\n",
      "73-th step: loss = 0.688920\n",
      "74-th step: loss = 0.688917\n",
      "75-th step: loss = 0.688915\n",
      "76-th step: loss = 0.688913\n",
      "77-th step: loss = 0.688911\n",
      "78-th step: loss = 0.688910\n",
      "79-th step: loss = 0.688908\n",
      "80-th step: loss = 0.688906\n",
      "80-th step: val. loss = 0.696940\n",
      "81-th step: loss = 0.688904\n",
      "82-th step: loss = 0.688902\n",
      "83-th step: loss = 0.688901\n",
      "84-th step: loss = 0.688899\n",
      "85-th step: loss = 0.688897\n",
      "86-th step: loss = 0.688895\n",
      "87-th step: loss = 0.688894\n",
      "88-th step: loss = 0.688891\n",
      "89-th step: loss = 0.688890\n",
      "90-th step: loss = 0.688888\n",
      "90-th step: val. loss = 0.696949\n",
      "91-th step: loss = 0.688886\n",
      "92-th step: loss = 0.688884\n",
      "93-th step: loss = 0.688882\n",
      "94-th step: loss = 0.688879\n",
      "95-th step: loss = 0.688877\n",
      "96-th step: loss = 0.688875\n",
      "97-th step: loss = 0.688873\n",
      "98-th step: loss = 0.688871\n",
      "99-th step: loss = 0.688869\n",
      "100-th step: loss = 0.688867\n",
      "100-th step: val. loss = 0.696968\n",
      "101-th step: loss = 0.688865\n",
      "102-th step: loss = 0.688862\n",
      "103-th step: loss = 0.688860\n",
      "104-th step: loss = 0.688858\n",
      "105-th step: loss = 0.688856\n",
      "106-th step: loss = 0.688854\n",
      "107-th step: loss = 0.688852\n",
      "108-th step: loss = 0.688850\n",
      "109-th step: loss = 0.688847\n",
      "110-th step: loss = 0.688845\n",
      "110-th step: val. loss = 0.697001\n",
      "111-th step: loss = 0.688843\n",
      "112-th step: loss = 0.688840\n",
      "113-th step: loss = 0.688839\n",
      "114-th step: loss = 0.688836\n",
      "115-th step: loss = 0.688834\n",
      "116-th step: loss = 0.688833\n",
      "117-th step: loss = 0.688830\n",
      "118-th step: loss = 0.688829\n",
      "119-th step: loss = 0.688826\n",
      "120-th step: loss = 0.688824\n",
      "120-th step: val. loss = 0.696978\n",
      "121-th step: loss = 0.688823\n",
      "122-th step: loss = 0.688821\n",
      "123-th step: loss = 0.688819\n",
      "124-th step: loss = 0.688816\n",
      "125-th step: loss = 0.688815\n",
      "126-th step: loss = 0.688813\n",
      "127-th step: loss = 0.688811\n",
      "128-th step: loss = 0.688809\n",
      "129-th step: loss = 0.688807\n",
      "130-th step: loss = 0.688806\n",
      "130-th step: val. loss = 0.696968\n",
      "131-th step: loss = 0.688804\n",
      "132-th step: loss = 0.688802\n",
      "133-th step: loss = 0.688800\n",
      "134-th step: loss = 0.688798\n",
      "135-th step: loss = 0.688796\n",
      "136-th step: loss = 0.688794\n",
      "137-th step: loss = 0.688792\n",
      "138-th step: loss = 0.688789\n",
      "139-th step: loss = 0.688787\n",
      "140-th step: loss = 0.688785\n",
      "140-th step: val. loss = 0.696982\n",
      "141-th step: loss = 0.688783\n",
      "142-th step: loss = 0.688781\n",
      "143-th step: loss = 0.688779\n",
      "144-th step: loss = 0.688776\n",
      "145-th step: loss = 0.688774\n",
      "146-th step: loss = 0.688771\n",
      "147-th step: loss = 0.688769\n",
      "148-th step: loss = 0.688766\n",
      "149-th step: loss = 0.688764\n",
      "150-th step: loss = 0.688761\n",
      "150-th step: val. loss = 0.697010\n",
      "151-th step: loss = 0.688758\n",
      "152-th step: loss = 0.688756\n",
      "153-th step: loss = 0.688754\n",
      "154-th step: loss = 0.688752\n",
      "155-th step: loss = 0.688750\n",
      "156-th step: loss = 0.688747\n",
      "157-th step: loss = 0.688745\n",
      "158-th step: loss = 0.688743\n",
      "159-th step: loss = 0.688740\n",
      "160-th step: loss = 0.688738\n",
      "160-th step: val. loss = 0.697066\n",
      "161-th step: loss = 0.688736\n",
      "162-th step: loss = 0.688733\n",
      "163-th step: loss = 0.688731\n",
      "164-th step: loss = 0.688729\n",
      "165-th step: loss = 0.688727\n",
      "166-th step: loss = 0.688725\n",
      "167-th step: loss = 0.688722\n",
      "168-th step: loss = 0.688720\n",
      "169-th step: loss = 0.688717\n",
      "170-th step: loss = 0.688715\n",
      "170-th step: val. loss = 0.697106\n",
      "171-th step: loss = 0.688712\n",
      "172-th step: loss = 0.688710\n",
      "173-th step: loss = 0.688707\n",
      "174-th step: loss = 0.688704\n",
      "175-th step: loss = 0.688701\n",
      "176-th step: loss = 0.688699\n",
      "177-th step: loss = 0.688696\n",
      "178-th step: loss = 0.688694\n",
      "179-th step: loss = 0.688692\n",
      "180-th step: loss = 0.688690\n",
      "180-th step: val. loss = 0.697113\n",
      "181-th step: loss = 0.688688\n",
      "182-th step: loss = 0.688687\n",
      "183-th step: loss = 0.688685\n",
      "184-th step: loss = 0.688683\n",
      "185-th step: loss = 0.688681\n",
      "186-th step: loss = 0.688679\n",
      "187-th step: loss = 0.688677\n",
      "188-th step: loss = 0.688675\n",
      "189-th step: loss = 0.688674\n",
      "190-th step: loss = 0.688672\n",
      "190-th step: val. loss = 0.697112\n",
      "191-th step: loss = 0.688671\n",
      "192-th step: loss = 0.688670\n",
      "193-th step: loss = 0.688668\n",
      "194-th step: loss = 0.688666\n",
      "195-th step: loss = 0.688665\n",
      "196-th step: loss = 0.688663\n",
      "197-th step: loss = 0.688662\n",
      "198-th step: loss = 0.688660\n",
      "199-th step: loss = 0.688658\n",
      "200-th step: loss = 0.688657\n",
      "200-th step: val. loss = 0.697131\n",
      "201-th step: loss = 0.688655\n",
      "202-th step: loss = 0.688654\n",
      "203-th step: loss = 0.688652\n",
      "204-th step: loss = 0.688650\n",
      "205-th step: loss = 0.688648\n",
      "206-th step: loss = 0.688647\n",
      "207-th step: loss = 0.688645\n",
      "208-th step: loss = 0.688644\n",
      "209-th step: loss = 0.688642\n",
      "210-th step: loss = 0.688641\n",
      "210-th step: val. loss = 0.697152\n",
      "211-th step: loss = 0.688640\n",
      "212-th step: loss = 0.688638\n",
      "213-th step: loss = 0.688636\n",
      "214-th step: loss = 0.688635\n",
      "215-th step: loss = 0.688634\n",
      "216-th step: loss = 0.688632\n",
      "217-th step: loss = 0.688631\n",
      "218-th step: loss = 0.688630\n",
      "219-th step: loss = 0.688629\n",
      "220-th step: loss = 0.688627\n",
      "220-th step: val. loss = 0.697183\n",
      "221-th step: loss = 0.688625\n",
      "222-th step: loss = 0.688624\n",
      "223-th step: loss = 0.688622\n",
      "224-th step: loss = 0.688621\n",
      "225-th step: loss = 0.688620\n",
      "226-th step: loss = 0.688619\n",
      "227-th step: loss = 0.688618\n",
      "228-th step: loss = 0.688616\n",
      "229-th step: loss = 0.688615\n",
      "230-th step: loss = 0.688614\n",
      "230-th step: val. loss = 0.697200\n",
      "231-th step: loss = 0.688613\n",
      "232-th step: loss = 0.688611\n",
      "233-th step: loss = 0.688610\n",
      "234-th step: loss = 0.688608\n",
      "235-th step: loss = 0.688606\n",
      "236-th step: loss = 0.688605\n",
      "237-th step: loss = 0.688603\n",
      "238-th step: loss = 0.688602\n",
      "239-th step: loss = 0.688601\n",
      "240-th step: loss = 0.688599\n",
      "240-th step: val. loss = 0.697234\n",
      "241-th step: loss = 0.688598\n",
      "242-th step: loss = 0.688596\n",
      "243-th step: loss = 0.688595\n",
      "244-th step: loss = 0.688594\n",
      "245-th step: loss = 0.688592\n",
      "246-th step: loss = 0.688591\n",
      "247-th step: loss = 0.688589\n",
      "248-th step: loss = 0.688587\n",
      "249-th step: loss = 0.688586\n",
      "250-th step: loss = 0.688585\n",
      "250-th step: val. loss = 0.697221\n",
      "251-th step: loss = 0.688583\n",
      "252-th step: loss = 0.688581\n",
      "253-th step: loss = 0.688580\n",
      "254-th step: loss = 0.688579\n",
      "255-th step: loss = 0.688577\n",
      "256-th step: loss = 0.688575\n",
      "257-th step: loss = 0.688574\n",
      "258-th step: loss = 0.688573\n",
      "259-th step: loss = 0.688572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260-th step: loss = 0.688570\n",
      "260-th step: val. loss = 0.697233\n",
      "261-th step: loss = 0.688569\n",
      "262-th step: loss = 0.688567\n",
      "263-th step: loss = 0.688566\n",
      "264-th step: loss = 0.688564\n",
      "265-th step: loss = 0.688563\n",
      "266-th step: loss = 0.688561\n",
      "267-th step: loss = 0.688559\n",
      "268-th step: loss = 0.688557\n",
      "269-th step: loss = 0.688555\n",
      "270-th step: loss = 0.688553\n",
      "270-th step: val. loss = 0.697257\n",
      "271-th step: loss = 0.688552\n",
      "272-th step: loss = 0.688550\n",
      "273-th step: loss = 0.688548\n",
      "274-th step: loss = 0.688546\n",
      "275-th step: loss = 0.688544\n",
      "276-th step: loss = 0.688542\n",
      "277-th step: loss = 0.688540\n",
      "278-th step: loss = 0.688539\n",
      "279-th step: loss = 0.688537\n",
      "280-th step: loss = 0.688535\n",
      "280-th step: val. loss = 0.697262\n",
      "281-th step: loss = 0.688534\n",
      "282-th step: loss = 0.688532\n",
      "283-th step: loss = 0.688529\n",
      "284-th step: loss = 0.688527\n",
      "285-th step: loss = 0.688525\n",
      "286-th step: loss = 0.688523\n",
      "287-th step: loss = 0.688520\n",
      "288-th step: loss = 0.688518\n",
      "289-th step: loss = 0.688516\n",
      "290-th step: loss = 0.688513\n",
      "290-th step: val. loss = 0.697266\n",
      "291-th step: loss = 0.688511\n",
      "292-th step: loss = 0.688508\n",
      "293-th step: loss = 0.688505\n",
      "294-th step: loss = 0.688503\n",
      "295-th step: loss = 0.688500\n",
      "296-th step: loss = 0.688498\n",
      "297-th step: loss = 0.688496\n",
      "298-th step: loss = 0.688493\n",
      "299-th step: loss = 0.688491\n",
      "300-th step: loss = 0.688488\n",
      "300-th step: val. loss = 0.697257\n",
      "301-th step: loss = 0.688486\n",
      "302-th step: loss = 0.688483\n",
      "303-th step: loss = 0.688480\n",
      "304-th step: loss = 0.688478\n",
      "305-th step: loss = 0.688475\n",
      "306-th step: loss = 0.688473\n",
      "307-th step: loss = 0.688470\n",
      "308-th step: loss = 0.688468\n",
      "309-th step: loss = 0.688466\n",
      "310-th step: loss = 0.688464\n",
      "310-th step: val. loss = 0.697277\n",
      "311-th step: loss = 0.688461\n",
      "312-th step: loss = 0.688459\n",
      "313-th step: loss = 0.688457\n",
      "314-th step: loss = 0.688454\n",
      "315-th step: loss = 0.688452\n",
      "316-th step: loss = 0.688449\n",
      "317-th step: loss = 0.688447\n",
      "318-th step: loss = 0.688444\n",
      "319-th step: loss = 0.688441\n",
      "320-th step: loss = 0.688439\n",
      "320-th step: val. loss = 0.697313\n",
      "321-th step: loss = 0.688436\n",
      "322-th step: loss = 0.688434\n",
      "323-th step: loss = 0.688431\n",
      "324-th step: loss = 0.688429\n",
      "325-th step: loss = 0.688427\n",
      "326-th step: loss = 0.688424\n",
      "327-th step: loss = 0.688421\n",
      "328-th step: loss = 0.688419\n",
      "329-th step: loss = 0.688417\n",
      "330-th step: loss = 0.688414\n",
      "330-th step: val. loss = 0.697356\n",
      "331-th step: loss = 0.688412\n",
      "332-th step: loss = 0.688409\n",
      "333-th step: loss = 0.688407\n",
      "334-th step: loss = 0.688404\n",
      "335-th step: loss = 0.688401\n",
      "336-th step: loss = 0.688398\n",
      "337-th step: loss = 0.688395\n",
      "338-th step: loss = 0.688394\n",
      "339-th step: loss = 0.688390\n",
      "340-th step: loss = 0.688387\n",
      "340-th step: val. loss = 0.697323\n",
      "341-th step: loss = 0.688385\n",
      "342-th step: loss = 0.688382\n",
      "343-th step: loss = 0.688379\n",
      "344-th step: loss = 0.688376\n",
      "345-th step: loss = 0.688374\n",
      "346-th step: loss = 0.688371\n",
      "347-th step: loss = 0.688369\n",
      "348-th step: loss = 0.688366\n",
      "349-th step: loss = 0.688364\n",
      "350-th step: loss = 0.688361\n",
      "350-th step: val. loss = 0.697343\n",
      "351-th step: loss = 0.688359\n",
      "352-th step: loss = 0.688356\n",
      "353-th step: loss = 0.688353\n",
      "354-th step: loss = 0.688350\n",
      "355-th step: loss = 0.688347\n",
      "356-th step: loss = 0.688344\n",
      "357-th step: loss = 0.688341\n",
      "358-th step: loss = 0.688339\n",
      "359-th step: loss = 0.688336\n",
      "360-th step: loss = 0.688334\n",
      "360-th step: val. loss = 0.697405\n",
      "361-th step: loss = 0.688331\n",
      "362-th step: loss = 0.688328\n",
      "363-th step: loss = 0.688325\n",
      "364-th step: loss = 0.688322\n",
      "365-th step: loss = 0.688320\n",
      "366-th step: loss = 0.688317\n",
      "367-th step: loss = 0.688314\n",
      "368-th step: loss = 0.688312\n",
      "369-th step: loss = 0.688309\n",
      "370-th step: loss = 0.688305\n",
      "370-th step: val. loss = 0.697469\n",
      "371-th step: loss = 0.688302\n",
      "372-th step: loss = 0.688299\n",
      "373-th step: loss = 0.688296\n",
      "374-th step: loss = 0.688293\n",
      "375-th step: loss = 0.688290\n",
      "376-th step: loss = 0.688287\n",
      "377-th step: loss = 0.688285\n",
      "378-th step: loss = 0.688282\n",
      "379-th step: loss = 0.688278\n",
      "380-th step: loss = 0.688275\n",
      "380-th step: val. loss = 0.697463\n",
      "381-th step: loss = 0.688273\n",
      "382-th step: loss = 0.688270\n",
      "383-th step: loss = 0.688268\n",
      "384-th step: loss = 0.688265\n",
      "385-th step: loss = 0.688262\n",
      "386-th step: loss = 0.688259\n",
      "387-th step: loss = 0.688256\n",
      "388-th step: loss = 0.688253\n",
      "389-th step: loss = 0.688250\n",
      "390-th step: loss = 0.688247\n",
      "390-th step: val. loss = 0.697433\n",
      "391-th step: loss = 0.688245\n",
      "392-th step: loss = 0.688243\n",
      "393-th step: loss = 0.688239\n",
      "394-th step: loss = 0.688237\n",
      "395-th step: loss = 0.688235\n",
      "396-th step: loss = 0.688233\n",
      "397-th step: loss = 0.688231\n",
      "398-th step: loss = 0.688228\n",
      "399-th step: loss = 0.688226\n",
      "400-th step: loss = 0.688224\n",
      "400-th step: val. loss = 0.697455\n",
      "401-th step: loss = 0.688221\n",
      "402-th step: loss = 0.688219\n",
      "403-th step: loss = 0.688216\n",
      "404-th step: loss = 0.688214\n",
      "405-th step: loss = 0.688212\n",
      "406-th step: loss = 0.688209\n",
      "407-th step: loss = 0.688208\n",
      "408-th step: loss = 0.688205\n",
      "409-th step: loss = 0.688203\n",
      "410-th step: loss = 0.688201\n",
      "410-th step: val. loss = 0.697482\n",
      "411-th step: loss = 0.688199\n",
      "412-th step: loss = 0.688197\n",
      "413-th step: loss = 0.688195\n",
      "414-th step: loss = 0.688193\n",
      "415-th step: loss = 0.688191\n",
      "416-th step: loss = 0.688190\n",
      "417-th step: loss = 0.688188\n",
      "418-th step: loss = 0.688186\n",
      "419-th step: loss = 0.688185\n",
      "420-th step: loss = 0.688182\n",
      "420-th step: val. loss = 0.697501\n",
      "421-th step: loss = 0.688181\n",
      "422-th step: loss = 0.688179\n",
      "423-th step: loss = 0.688177\n",
      "424-th step: loss = 0.688176\n",
      "425-th step: loss = 0.688174\n",
      "426-th step: loss = 0.688172\n",
      "427-th step: loss = 0.688170\n",
      "428-th step: loss = 0.688168\n",
      "429-th step: loss = 0.688166\n",
      "430-th step: loss = 0.688164\n",
      "430-th step: val. loss = 0.697517\n",
      "431-th step: loss = 0.688161\n",
      "432-th step: loss = 0.688159\n",
      "433-th step: loss = 0.688156\n",
      "434-th step: loss = 0.688154\n",
      "435-th step: loss = 0.688152\n",
      "436-th step: loss = 0.688150\n",
      "437-th step: loss = 0.688148\n",
      "438-th step: loss = 0.688145\n",
      "439-th step: loss = 0.688142\n",
      "440-th step: loss = 0.688141\n",
      "440-th step: val. loss = 0.697566\n",
      "441-th step: loss = 0.688138\n",
      "442-th step: loss = 0.688136\n",
      "443-th step: loss = 0.688134\n",
      "444-th step: loss = 0.688131\n",
      "445-th step: loss = 0.688129\n",
      "446-th step: loss = 0.688128\n",
      "447-th step: loss = 0.688125\n",
      "448-th step: loss = 0.688123\n",
      "449-th step: loss = 0.688121\n",
      "450-th step: loss = 0.688119\n",
      "450-th step: val. loss = 0.697596\n",
      "451-th step: loss = 0.688116\n",
      "452-th step: loss = 0.688114\n",
      "453-th step: loss = 0.688112\n",
      "454-th step: loss = 0.688111\n",
      "455-th step: loss = 0.688109\n",
      "456-th step: loss = 0.688107\n",
      "457-th step: loss = 0.688104\n",
      "458-th step: loss = 0.688103\n",
      "459-th step: loss = 0.688100\n",
      "460-th step: loss = 0.688099\n",
      "460-th step: val. loss = 0.697594\n",
      "461-th step: loss = 0.688097\n",
      "462-th step: loss = 0.688095\n",
      "463-th step: loss = 0.688093\n",
      "464-th step: loss = 0.688091\n",
      "465-th step: loss = 0.688089\n",
      "466-th step: loss = 0.688088\n",
      "467-th step: loss = 0.688086\n",
      "468-th step: loss = 0.688084\n",
      "469-th step: loss = 0.688082\n",
      "470-th step: loss = 0.688081\n",
      "470-th step: val. loss = 0.697590\n",
      "471-th step: loss = 0.688079\n",
      "472-th step: loss = 0.688077\n",
      "473-th step: loss = 0.688075\n",
      "474-th step: loss = 0.688073\n",
      "475-th step: loss = 0.688072\n",
      "476-th step: loss = 0.688071\n",
      "477-th step: loss = 0.688070\n",
      "478-th step: loss = 0.688068\n",
      "479-th step: loss = 0.688067\n",
      "480-th step: loss = 0.688065\n",
      "480-th step: val. loss = 0.697623\n",
      "481-th step: loss = 0.688064\n",
      "482-th step: loss = 0.688063\n",
      "483-th step: loss = 0.688061\n",
      "484-th step: loss = 0.688060\n",
      "485-th step: loss = 0.688058\n",
      "486-th step: loss = 0.688057\n",
      "487-th step: loss = 0.688056\n",
      "488-th step: loss = 0.688055\n",
      "489-th step: loss = 0.688053\n",
      "490-th step: loss = 0.688052\n",
      "490-th step: val. loss = 0.697646\n",
      "491-th step: loss = 0.688051\n",
      "492-th step: loss = 0.688051\n",
      "493-th step: loss = 0.688050\n",
      "494-th step: loss = 0.688049\n",
      "495-th step: loss = 0.688048\n",
      "496-th step: loss = 0.688046\n",
      "497-th step: loss = 0.688046\n",
      "498-th step: loss = 0.688044\n",
      "499-th step: loss = 0.688043\n",
      "500-th step: loss = 0.688043\n",
      "500-th step: val. loss = 0.697656\n",
      "501-th step: loss = 0.688041\n",
      "502-th step: loss = 0.688040\n",
      "503-th step: loss = 0.688039\n",
      "504-th step: loss = 0.688038\n",
      "505-th step: loss = 0.688038\n",
      "506-th step: loss = 0.688036\n",
      "507-th step: loss = 0.688035\n",
      "508-th step: loss = 0.688034\n",
      "509-th step: loss = 0.688033\n",
      "510-th step: loss = 0.688032\n",
      "510-th step: val. loss = 0.697656\n",
      "511-th step: loss = 0.688029\n",
      "512-th step: loss = 0.688030\n",
      "513-th step: loss = 0.688028\n",
      "514-th step: loss = 0.688026\n",
      "515-th step: loss = 0.688025\n",
      "516-th step: loss = 0.688024\n",
      "517-th step: loss = 0.688022\n",
      "518-th step: loss = 0.688021\n",
      "519-th step: loss = 0.688019\n",
      "520-th step: loss = 0.688018\n",
      "520-th step: val. loss = 0.697706\n",
      "521-th step: loss = 0.688017\n",
      "522-th step: loss = 0.688015\n",
      "523-th step: loss = 0.688014\n",
      "524-th step: loss = 0.688013\n",
      "525-th step: loss = 0.688011\n",
      "526-th step: loss = 0.688010\n",
      "527-th step: loss = 0.688009\n",
      "528-th step: loss = 0.688008\n",
      "529-th step: loss = 0.688006\n",
      "530-th step: loss = 0.688005\n",
      "530-th step: val. loss = 0.697744\n",
      "531-th step: loss = 0.688003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532-th step: loss = 0.688002\n",
      "533-th step: loss = 0.688001\n",
      "534-th step: loss = 0.687999\n",
      "535-th step: loss = 0.687997\n",
      "536-th step: loss = 0.687997\n",
      "537-th step: loss = 0.687995\n",
      "538-th step: loss = 0.687993\n",
      "539-th step: loss = 0.687992\n",
      "540-th step: loss = 0.687990\n",
      "540-th step: val. loss = 0.697773\n",
      "541-th step: loss = 0.687990\n",
      "542-th step: loss = 0.687988\n",
      "543-th step: loss = 0.687986\n",
      "544-th step: loss = 0.687986\n",
      "545-th step: loss = 0.687984\n",
      "546-th step: loss = 0.687982\n",
      "547-th step: loss = 0.687982\n",
      "548-th step: loss = 0.687979\n",
      "549-th step: loss = 0.687978\n",
      "550-th step: loss = 0.687976\n",
      "550-th step: val. loss = 0.697788\n",
      "551-th step: loss = 0.687975\n",
      "552-th step: loss = 0.687973\n",
      "553-th step: loss = 0.687973\n",
      "554-th step: loss = 0.687971\n",
      "555-th step: loss = 0.687969\n",
      "556-th step: loss = 0.687968\n",
      "557-th step: loss = 0.687967\n",
      "558-th step: loss = 0.687965\n",
      "559-th step: loss = 0.687964\n",
      "560-th step: loss = 0.687963\n",
      "560-th step: val. loss = 0.697807\n",
      "561-th step: loss = 0.687961\n",
      "562-th step: loss = 0.687961\n",
      "563-th step: loss = 0.687958\n",
      "564-th step: loss = 0.687958\n",
      "565-th step: loss = 0.687956\n",
      "566-th step: loss = 0.687955\n",
      "567-th step: loss = 0.687954\n",
      "568-th step: loss = 0.687952\n",
      "569-th step: loss = 0.687951\n",
      "570-th step: loss = 0.687949\n",
      "570-th step: val. loss = 0.697837\n",
      "571-th step: loss = 0.687948\n",
      "572-th step: loss = 0.687946\n",
      "573-th step: loss = 0.687945\n",
      "574-th step: loss = 0.687944\n",
      "575-th step: loss = 0.687943\n",
      "576-th step: loss = 0.687942\n",
      "577-th step: loss = 0.687940\n",
      "578-th step: loss = 0.687939\n",
      "579-th step: loss = 0.687938\n",
      "580-th step: loss = 0.687937\n",
      "580-th step: val. loss = 0.697821\n",
      "581-th step: loss = 0.687936\n",
      "582-th step: loss = 0.687935\n",
      "583-th step: loss = 0.687934\n",
      "584-th step: loss = 0.687933\n",
      "585-th step: loss = 0.687932\n",
      "586-th step: loss = 0.687931\n",
      "587-th step: loss = 0.687930\n",
      "588-th step: loss = 0.687931\n",
      "589-th step: loss = 0.687928\n",
      "590-th step: loss = 0.687928\n",
      "590-th step: val. loss = 0.697847\n",
      "591-th step: loss = 0.687928\n",
      "592-th step: loss = 0.687926\n",
      "593-th step: loss = 0.687926\n",
      "594-th step: loss = 0.687925\n",
      "595-th step: loss = 0.687922\n",
      "596-th step: loss = 0.687924\n",
      "597-th step: loss = 0.687922\n",
      "598-th step: loss = 0.687920\n",
      "599-th step: loss = 0.687920\n",
      "600-th step: loss = 0.687918\n",
      "600-th step: val. loss = 0.697870\n",
      "601-th step: loss = 0.687917\n",
      "602-th step: loss = 0.687916\n",
      "603-th step: loss = 0.687915\n",
      "604-th step: loss = 0.687914\n",
      "605-th step: loss = 0.687912\n",
      "606-th step: loss = 0.687911\n",
      "607-th step: loss = 0.687909\n",
      "608-th step: loss = 0.687908\n",
      "609-th step: loss = 0.687907\n",
      "610-th step: loss = 0.687906\n",
      "610-th step: val. loss = 0.697864\n",
      "611-th step: loss = 0.687906\n",
      "612-th step: loss = 0.687904\n",
      "613-th step: loss = 0.687903\n",
      "614-th step: loss = 0.687902\n",
      "615-th step: loss = 0.687901\n",
      "616-th step: loss = 0.687899\n",
      "617-th step: loss = 0.687898\n",
      "618-th step: loss = 0.687897\n",
      "619-th step: loss = 0.687896\n",
      "620-th step: loss = 0.687894\n",
      "620-th step: val. loss = 0.697873\n",
      "621-th step: loss = 0.687893\n",
      "622-th step: loss = 0.687892\n",
      "623-th step: loss = 0.687892\n",
      "624-th step: loss = 0.687891\n",
      "625-th step: loss = 0.687889\n",
      "626-th step: loss = 0.687887\n",
      "627-th step: loss = 0.687887\n",
      "628-th step: loss = 0.687886\n",
      "629-th step: loss = 0.687886\n",
      "630-th step: loss = 0.687884\n",
      "630-th step: val. loss = 0.697878\n",
      "631-th step: loss = 0.687884\n",
      "632-th step: loss = 0.687882\n",
      "633-th step: loss = 0.687883\n",
      "634-th step: loss = 0.687881\n",
      "635-th step: loss = 0.687879\n",
      "636-th step: loss = 0.687878\n",
      "637-th step: loss = 0.687879\n",
      "638-th step: loss = 0.687877\n",
      "639-th step: loss = 0.687875\n",
      "640-th step: loss = 0.687874\n",
      "640-th step: val. loss = 0.697909\n",
      "641-th step: loss = 0.687874\n",
      "642-th step: loss = 0.687873\n",
      "643-th step: loss = 0.687871\n",
      "644-th step: loss = 0.687871\n",
      "645-th step: loss = 0.687870\n",
      "646-th step: loss = 0.687869\n",
      "647-th step: loss = 0.687870\n",
      "648-th step: loss = 0.687868\n",
      "649-th step: loss = 0.687867\n",
      "650-th step: loss = 0.687866\n",
      "650-th step: val. loss = 0.697903\n",
      "651-th step: loss = 0.687865\n",
      "652-th step: loss = 0.687864\n",
      "653-th step: loss = 0.687863\n",
      "654-th step: loss = 0.687862\n",
      "655-th step: loss = 0.687861\n",
      "656-th step: loss = 0.687860\n",
      "657-th step: loss = 0.687860\n",
      "658-th step: loss = 0.687859\n",
      "659-th step: loss = 0.687858\n",
      "660-th step: loss = 0.687857\n",
      "660-th step: val. loss = 0.697931\n",
      "661-th step: loss = 0.687856\n",
      "662-th step: loss = 0.687856\n",
      "663-th step: loss = 0.687855\n",
      "664-th step: loss = 0.687854\n",
      "665-th step: loss = 0.687853\n",
      "666-th step: loss = 0.687852\n",
      "667-th step: loss = 0.687851\n",
      "668-th step: loss = 0.687850\n",
      "669-th step: loss = 0.687849\n",
      "670-th step: loss = 0.687848\n",
      "670-th step: val. loss = 0.697923\n",
      "671-th step: loss = 0.687847\n",
      "672-th step: loss = 0.687847\n",
      "673-th step: loss = 0.687845\n",
      "674-th step: loss = 0.687845\n",
      "675-th step: loss = 0.687844\n",
      "676-th step: loss = 0.687842\n",
      "677-th step: loss = 0.687842\n",
      "678-th step: loss = 0.687841\n",
      "679-th step: loss = 0.687840\n",
      "680-th step: loss = 0.687839\n",
      "680-th step: val. loss = 0.697890\n",
      "681-th step: loss = 0.687839\n",
      "682-th step: loss = 0.687838\n",
      "683-th step: loss = 0.687837\n",
      "684-th step: loss = 0.687836\n",
      "685-th step: loss = 0.687835\n",
      "686-th step: loss = 0.687834\n",
      "687-th step: loss = 0.687833\n",
      "688-th step: loss = 0.687832\n",
      "689-th step: loss = 0.687831\n",
      "690-th step: loss = 0.687831\n",
      "690-th step: val. loss = 0.697916\n",
      "691-th step: loss = 0.687830\n",
      "692-th step: loss = 0.687829\n",
      "693-th step: loss = 0.687828\n",
      "694-th step: loss = 0.687827\n",
      "695-th step: loss = 0.687827\n",
      "696-th step: loss = 0.687826\n",
      "697-th step: loss = 0.687825\n",
      "698-th step: loss = 0.687825\n",
      "699-th step: loss = 0.687823\n",
      "700-th step: loss = 0.687823\n",
      "700-th step: val. loss = 0.697932\n",
      "701-th step: loss = 0.687822\n",
      "702-th step: loss = 0.687822\n",
      "703-th step: loss = 0.687821\n",
      "704-th step: loss = 0.687820\n",
      "705-th step: loss = 0.687819\n",
      "706-th step: loss = 0.687820\n",
      "707-th step: loss = 0.687817\n",
      "708-th step: loss = 0.687817\n",
      "709-th step: loss = 0.687816\n",
      "710-th step: loss = 0.687815\n",
      "710-th step: val. loss = 0.697972\n",
      "711-th step: loss = 0.687814\n",
      "712-th step: loss = 0.687813\n",
      "713-th step: loss = 0.687813\n",
      "714-th step: loss = 0.687811\n",
      "715-th step: loss = 0.687811\n",
      "716-th step: loss = 0.687810\n",
      "717-th step: loss = 0.687810\n",
      "718-th step: loss = 0.687808\n",
      "719-th step: loss = 0.687808\n",
      "720-th step: loss = 0.687808\n",
      "720-th step: val. loss = 0.697979\n",
      "721-th step: loss = 0.687806\n",
      "722-th step: loss = 0.687805\n",
      "723-th step: loss = 0.687805\n",
      "724-th step: loss = 0.687803\n",
      "725-th step: loss = 0.687803\n",
      "726-th step: loss = 0.687802\n",
      "727-th step: loss = 0.687800\n",
      "728-th step: loss = 0.687799\n",
      "729-th step: loss = 0.687798\n",
      "730-th step: loss = 0.687797\n",
      "730-th step: val. loss = 0.697980\n",
      "731-th step: loss = 0.687796\n",
      "732-th step: loss = 0.687795\n",
      "733-th step: loss = 0.687794\n",
      "734-th step: loss = 0.687793\n",
      "735-th step: loss = 0.687792\n",
      "736-th step: loss = 0.687791\n",
      "737-th step: loss = 0.687790\n",
      "738-th step: loss = 0.687789\n",
      "739-th step: loss = 0.687787\n",
      "740-th step: loss = 0.687787\n",
      "740-th step: val. loss = 0.698017\n",
      "741-th step: loss = 0.687786\n",
      "742-th step: loss = 0.687785\n",
      "743-th step: loss = 0.687784\n",
      "744-th step: loss = 0.687783\n",
      "745-th step: loss = 0.687781\n",
      "746-th step: loss = 0.687781\n",
      "747-th step: loss = 0.687779\n",
      "748-th step: loss = 0.687778\n",
      "749-th step: loss = 0.687777\n",
      "750-th step: loss = 0.687775\n",
      "750-th step: val. loss = 0.698058\n",
      "751-th step: loss = 0.687774\n",
      "752-th step: loss = 0.687773\n",
      "753-th step: loss = 0.687772\n",
      "754-th step: loss = 0.687771\n",
      "755-th step: loss = 0.687769\n",
      "756-th step: loss = 0.687768\n",
      "757-th step: loss = 0.687767\n",
      "758-th step: loss = 0.687765\n",
      "759-th step: loss = 0.687764\n",
      "760-th step: loss = 0.687763\n",
      "760-th step: val. loss = 0.698070\n",
      "761-th step: loss = 0.687761\n",
      "762-th step: loss = 0.687760\n",
      "763-th step: loss = 0.687758\n",
      "764-th step: loss = 0.687757\n",
      "765-th step: loss = 0.687755\n",
      "766-th step: loss = 0.687753\n",
      "767-th step: loss = 0.687752\n",
      "768-th step: loss = 0.687750\n",
      "769-th step: loss = 0.687749\n",
      "770-th step: loss = 0.687747\n",
      "770-th step: val. loss = 0.698090\n",
      "771-th step: loss = 0.687745\n",
      "772-th step: loss = 0.687744\n",
      "773-th step: loss = 0.687742\n",
      "774-th step: loss = 0.687740\n",
      "775-th step: loss = 0.687738\n",
      "776-th step: loss = 0.687737\n",
      "777-th step: loss = 0.687735\n",
      "778-th step: loss = 0.687734\n",
      "779-th step: loss = 0.687733\n",
      "780-th step: loss = 0.687732\n",
      "780-th step: val. loss = 0.698094\n",
      "781-th step: loss = 0.687729\n",
      "782-th step: loss = 0.687729\n",
      "783-th step: loss = 0.687727\n",
      "784-th step: loss = 0.687726\n",
      "785-th step: loss = 0.687724\n",
      "786-th step: loss = 0.687723\n",
      "787-th step: loss = 0.687722\n",
      "788-th step: loss = 0.687720\n",
      "789-th step: loss = 0.687719\n",
      "790-th step: loss = 0.687718\n",
      "790-th step: val. loss = 0.698126\n",
      "791-th step: loss = 0.687716\n",
      "792-th step: loss = 0.687715\n",
      "793-th step: loss = 0.687713\n",
      "794-th step: loss = 0.687712\n",
      "795-th step: loss = 0.687710\n",
      "796-th step: loss = 0.687709\n",
      "797-th step: loss = 0.687706\n",
      "798-th step: loss = 0.687704\n",
      "799-th step: loss = 0.687703\n",
      "800-th step: loss = 0.687700\n",
      "800-th step: val. loss = 0.698135\n",
      "801-th step: loss = 0.687697\n",
      "802-th step: loss = 0.687696\n",
      "803-th step: loss = 0.687694\n",
      "804-th step: loss = 0.687692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805-th step: loss = 0.687690\n",
      "806-th step: loss = 0.687688\n",
      "807-th step: loss = 0.687687\n",
      "808-th step: loss = 0.687685\n",
      "809-th step: loss = 0.687684\n",
      "810-th step: loss = 0.687683\n",
      "810-th step: val. loss = 0.698130\n",
      "811-th step: loss = 0.687681\n",
      "812-th step: loss = 0.687679\n",
      "813-th step: loss = 0.687678\n",
      "814-th step: loss = 0.687676\n",
      "815-th step: loss = 0.687674\n",
      "816-th step: loss = 0.687672\n",
      "817-th step: loss = 0.687671\n",
      "818-th step: loss = 0.687669\n",
      "819-th step: loss = 0.687667\n",
      "820-th step: loss = 0.687665\n",
      "820-th step: val. loss = 0.698130\n",
      "821-th step: loss = 0.687664\n",
      "822-th step: loss = 0.687662\n",
      "823-th step: loss = 0.687661\n",
      "824-th step: loss = 0.687660\n",
      "825-th step: loss = 0.687658\n",
      "826-th step: loss = 0.687657\n",
      "827-th step: loss = 0.687656\n",
      "828-th step: loss = 0.687654\n",
      "829-th step: loss = 0.687653\n",
      "830-th step: loss = 0.687651\n",
      "830-th step: val. loss = 0.698129\n",
      "831-th step: loss = 0.687650\n",
      "832-th step: loss = 0.687649\n",
      "833-th step: loss = 0.687648\n",
      "834-th step: loss = 0.687647\n",
      "835-th step: loss = 0.687646\n",
      "836-th step: loss = 0.687645\n",
      "837-th step: loss = 0.687644\n",
      "838-th step: loss = 0.687643\n",
      "839-th step: loss = 0.687641\n",
      "840-th step: loss = 0.687641\n",
      "840-th step: val. loss = 0.698108\n",
      "841-th step: loss = 0.687641\n",
      "842-th step: loss = 0.687639\n",
      "843-th step: loss = 0.687639\n",
      "844-th step: loss = 0.687638\n",
      "845-th step: loss = 0.687635\n",
      "846-th step: loss = 0.687635\n",
      "847-th step: loss = 0.687635\n",
      "848-th step: loss = 0.687633\n",
      "849-th step: loss = 0.687632\n",
      "850-th step: loss = 0.687632\n",
      "850-th step: val. loss = 0.698178\n",
      "851-th step: loss = 0.687630\n",
      "852-th step: loss = 0.687630\n",
      "853-th step: loss = 0.687630\n",
      "854-th step: loss = 0.687628\n",
      "855-th step: loss = 0.687627\n",
      "856-th step: loss = 0.687627\n",
      "857-th step: loss = 0.687629\n",
      "858-th step: loss = 0.687628\n",
      "859-th step: loss = 0.687624\n",
      "860-th step: loss = 0.687623\n",
      "860-th step: val. loss = 0.698181\n",
      "861-th step: loss = 0.687625\n",
      "862-th step: loss = 0.687621\n",
      "863-th step: loss = 0.687622\n",
      "864-th step: loss = 0.687622\n",
      "865-th step: loss = 0.687620\n",
      "866-th step: loss = 0.687619\n",
      "867-th step: loss = 0.687620\n",
      "868-th step: loss = 0.687618\n",
      "869-th step: loss = 0.687616\n",
      "870-th step: loss = 0.687617\n",
      "870-th step: val. loss = 0.698255\n",
      "871-th step: loss = 0.687616\n",
      "872-th step: loss = 0.687613\n",
      "873-th step: loss = 0.687613\n",
      "874-th step: loss = 0.687612\n",
      "875-th step: loss = 0.687612\n",
      "876-th step: loss = 0.687610\n",
      "877-th step: loss = 0.687609\n",
      "878-th step: loss = 0.687609\n",
      "879-th step: loss = 0.687606\n",
      "880-th step: loss = 0.687607\n",
      "880-th step: val. loss = 0.698249\n",
      "881-th step: loss = 0.687606\n",
      "882-th step: loss = 0.687605\n",
      "883-th step: loss = 0.687604\n",
      "884-th step: loss = 0.687605\n",
      "885-th step: loss = 0.687604\n",
      "886-th step: loss = 0.687603\n",
      "887-th step: loss = 0.687601\n",
      "888-th step: loss = 0.687602\n",
      "889-th step: loss = 0.687600\n",
      "890-th step: loss = 0.687599\n",
      "890-th step: val. loss = 0.698277\n",
      "891-th step: loss = 0.687598\n",
      "892-th step: loss = 0.687598\n",
      "893-th step: loss = 0.687598\n",
      "894-th step: loss = 0.687596\n",
      "895-th step: loss = 0.687595\n",
      "896-th step: loss = 0.687594\n",
      "897-th step: loss = 0.687593\n",
      "898-th step: loss = 0.687595\n",
      "899-th step: loss = 0.687593\n",
      "900-th step: loss = 0.687591\n",
      "900-th step: val. loss = 0.698281\n",
      "901-th step: loss = 0.687590\n",
      "902-th step: loss = 0.687590\n",
      "903-th step: loss = 0.687589\n",
      "904-th step: loss = 0.687589\n",
      "905-th step: loss = 0.687588\n",
      "906-th step: loss = 0.687587\n",
      "907-th step: loss = 0.687586\n",
      "908-th step: loss = 0.687586\n",
      "909-th step: loss = 0.687586\n",
      "910-th step: loss = 0.687585\n",
      "910-th step: val. loss = 0.698315\n",
      "911-th step: loss = 0.687586\n",
      "912-th step: loss = 0.687583\n",
      "913-th step: loss = 0.687583\n",
      "914-th step: loss = 0.687581\n",
      "915-th step: loss = 0.687582\n",
      "916-th step: loss = 0.687581\n",
      "917-th step: loss = 0.687581\n",
      "918-th step: loss = 0.687579\n",
      "919-th step: loss = 0.687579\n",
      "920-th step: loss = 0.687578\n",
      "920-th step: val. loss = 0.698338\n",
      "921-th step: loss = 0.687577\n",
      "922-th step: loss = 0.687576\n",
      "923-th step: loss = 0.687575\n",
      "924-th step: loss = 0.687575\n",
      "925-th step: loss = 0.687574\n",
      "926-th step: loss = 0.687574\n",
      "927-th step: loss = 0.687574\n",
      "928-th step: loss = 0.687574\n",
      "929-th step: loss = 0.687571\n",
      "930-th step: loss = 0.687572\n",
      "930-th step: val. loss = 0.698323\n",
      "931-th step: loss = 0.687571\n",
      "932-th step: loss = 0.687569\n",
      "933-th step: loss = 0.687571\n",
      "934-th step: loss = 0.687570\n",
      "935-th step: loss = 0.687569\n",
      "936-th step: loss = 0.687571\n",
      "937-th step: loss = 0.687568\n",
      "938-th step: loss = 0.687567\n",
      "939-th step: loss = 0.687568\n",
      "940-th step: loss = 0.687565\n",
      "940-th step: val. loss = 0.698356\n",
      "941-th step: loss = 0.687564\n",
      "942-th step: loss = 0.687564\n",
      "943-th step: loss = 0.687564\n",
      "944-th step: loss = 0.687561\n",
      "945-th step: loss = 0.687562\n",
      "946-th step: loss = 0.687561\n",
      "947-th step: loss = 0.687560\n",
      "948-th step: loss = 0.687559\n",
      "949-th step: loss = 0.687558\n",
      "950-th step: loss = 0.687557\n",
      "950-th step: val. loss = 0.698390\n",
      "951-th step: loss = 0.687556\n",
      "952-th step: loss = 0.687555\n",
      "953-th step: loss = 0.687555\n",
      "954-th step: loss = 0.687554\n",
      "955-th step: loss = 0.687553\n",
      "956-th step: loss = 0.687552\n",
      "957-th step: loss = 0.687553\n",
      "958-th step: loss = 0.687557\n",
      "959-th step: loss = 0.687550\n",
      "960-th step: loss = 0.687551\n",
      "960-th step: val. loss = 0.698404\n",
      "961-th step: loss = 0.687554\n",
      "962-th step: loss = 0.687549\n",
      "963-th step: loss = 0.687548\n",
      "964-th step: loss = 0.687548\n",
      "965-th step: loss = 0.687548\n",
      "966-th step: loss = 0.687545\n",
      "967-th step: loss = 0.687546\n",
      "968-th step: loss = 0.687547\n",
      "969-th step: loss = 0.687544\n",
      "970-th step: loss = 0.687543\n",
      "970-th step: val. loss = 0.698471\n",
      "971-th step: loss = 0.687544\n",
      "972-th step: loss = 0.687542\n",
      "973-th step: loss = 0.687539\n",
      "974-th step: loss = 0.687540\n",
      "975-th step: loss = 0.687540\n",
      "976-th step: loss = 0.687537\n",
      "977-th step: loss = 0.687536\n",
      "978-th step: loss = 0.687537\n",
      "979-th step: loss = 0.687535\n",
      "980-th step: loss = 0.687534\n",
      "980-th step: val. loss = 0.698436\n",
      "981-th step: loss = 0.687533\n",
      "982-th step: loss = 0.687531\n",
      "983-th step: loss = 0.687531\n",
      "984-th step: loss = 0.687529\n",
      "985-th step: loss = 0.687530\n",
      "986-th step: loss = 0.687530\n",
      "987-th step: loss = 0.687530\n",
      "988-th step: loss = 0.687528\n",
      "989-th step: loss = 0.687526\n",
      "990-th step: loss = 0.687525\n",
      "990-th step: val. loss = 0.698474\n",
      "991-th step: loss = 0.687523\n",
      "992-th step: loss = 0.687523\n",
      "993-th step: loss = 0.687523\n",
      "994-th step: loss = 0.687522\n",
      "995-th step: loss = 0.687521\n",
      "996-th step: loss = 0.687520\n",
      "997-th step: loss = 0.687519\n",
      "998-th step: loss = 0.687518\n",
      "999-th step: loss = 0.687517\n"
     ]
    }
   ],
   "source": [
    "n_steps = 1000\n",
    "\n",
    "#reset the default graph to avoid the errors\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_model = TFModel(unet, logits2pred=softmax, logits2loss=softmax_cross_entropy, optimize=optimize)\n",
    "tf_model.load('train')\n",
    "for i in range(n_steps):\n",
    "    loss = tf_model.do_train_step(data[:100], labels[:100], lr=1e-3)\n",
    "    train_loss.append(loss)\n",
    "    print('%d-th step: loss = %f' % (i, loss))\n",
    "    if i % 10 == 0:\n",
    "        y_pred, loss = tf_model.do_val_step(data[100:], labels[100:])\n",
    "        val_loss.extend([loss]*10)\n",
    "        print('%d-th step: val. loss = %f' % (i, loss))\n",
    "# after the training is done\n",
    "tf_model.save('train')\n",
    "tf_model.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAE/CAYAAAA6+mr5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmcVNWd9/HPr1doFmn2VWgMbiAB\nQcZlNDqOBk2iMWbBmEWfiWYZzei8MpF5MjGok3lM4pI40SdDlklGjYSYGEmCoyYaonlwwhI3UARZ\npAWhQZqtEXr5PX+cW3R1UdVd1X27m779fb9e91V1b52699yG/vY5dznX3B0REWlW1N0VEBE52igY\nRUQyKBhFRDIoGEVEMigYRUQyKBhFRDIoGKXTmNkEM3MzK2nn9481s31mVhx33eJgZlea2RNxl5Xu\nZ7qOMXnMbCPwGXf/XTfXYwKwASh194burEsmM/sxUO3u/9LddZGjj1qMclRqbyszKduX7qVg7GXM\n7BozW2dmb5vZIjMbHS03M7vbzLab2W4ze9HMpkSfXWxmq81sr5m9aWZfyrHuYjO7w8x2mNl64H0Z\nn280s79Nm59nZg9E71Pd7r8zszeApzK74mb2BzO7zcz+FNXlCTMbmra+T5nZJjPbaWZfzdxeWrlr\ngSuBL0dd9V+n1e8mM3sR2G9mJWY218xej7a32swuS1vPVWb2bNq8m9nnzGytme0ys3vNzNpRttjM\n7ox+jhvM7LqOHJKQwikYexEz+xvg/wAfBUYBm4AF0ccXAucAxwODgI8BO6PPfgh81t0HAFOAp3Js\n4hrg/cB0YCbw4XZU8z3AScB7c3z+ceBqYDhQBnwp2reTgfsIgTcKOAYYk20F7j4feBD4prv3d/cP\npH18BSHQB0Xd/9eBs6P13QI8YGajWqn/+4HTgHcTfs659qO1stcAFwHTgFOBD7ayDukECsbe5Urg\nR+6+0t0PAv8MnBEdC6wHBgAnEo49v+LuW6Pv1QMnm9lAd9/l7itzrP+jwLfdfbO7v00I4ULNc/f9\n7n4gx+f/6e6vRZ8vJIQHhBD+tbs/6+6HgJuB9hxAvyeq/wEAd/+5u29x9yZ3/xmwFpjVyvdvd/da\nd38DeDqtfoWU/SjwHXevdvddwO3t2A/pAAVj7zKa0EoEwN33EVqFY9z9KeC7wL3ANjObb2YDo6KX\nAxcDm8xsiZmd0cr6N6fNb8pRrjWb2/j8rbT3dUD/bNt29zqaW7zt3n7UPX/ezGrNrJbQYh6a/aut\n1q+Qspk/x7Z+JhIzBWPvsgUYn5oxs37AEOBNAHe/x91nAJMJXep/ipYvc/dLCd3XXxFaatlsBcal\nzR+b8fl+oCJtfmSWdbT3MomtwNjUjJn1JexbLrm2c3i5mY0Hvg9cBwxx90HAy4C1s475arEvtPyZ\nShdQMCZXqZn1SZtKgJ8CV5vZNDMrB/4N+B9332hmp5nZX5lZKSHA3gEazawsugbvGHevB/YAjTm2\nuRD4opmNNbNKYG7G588Dc8ys1Mzaewwyl4eBD5jZmWZWRjge2FqAbQMmtrHOfoSgrAEws6sJLcbO\nthD4BzMbY2aDgJu6YJuSRsGYXIuBA2nTPHf/PfBV4BeEVslxwJyo/EBC62gXoQu8E7gj+uyTwEYz\n2wN8DvhEjm1+H3gceAFYCfwy4/OvRtvcRQiun3ZoD9O4+yrgesLJpK3AXmA7cDDHV35IOG5aa2a/\nyrHO1cCdwFJCkJ4C/CmuOrfi+8ATwIvAXwj/lg3k/oMkMdMF3pJIZtYfqAUmufuG7q5PR5jZRcD3\n3H18m4UlFmoxSmKY2QfMrCI6dnoH8BKwsXtrVTgz6xtdO1piZmOArwGPdHe9ehMFoyTJpYQTTFuA\nScAc75ldIiMcathF6Eq/Qrj8SLqIutIiIhnUYhQRyaBgFBHJcNTdlD506FCfMGFCd1dDRBJmxYoV\nO9x9WD5lj7pgnDBhAsuXL+/uaohIwphZ3reoqistIpJBwSgikkHBKCKS4ag7xijS29TX11NdXc07\n77zT3VVJhD59+jB27FhKS0vbvQ4Fo0g3q66uZsCAAUyYMIHo6QbSTu7Ozp07qa6upqqqqt3rUVda\npJu98847DBkyRKEYAzNjyJAhHW59KxhFjgIKxfjE8bNUMIr0crW1tdx3330Ff+/iiy+mtra2E2rU\n/RSMIr1crmBsbGx9XNzFixczaNCgzqpWt+r5wbhkCfw0toGgRXqduXPn8vrrrzNt2jROO+00zjvv\nPD7+8Y9zyimnAPDBD36QGTNmMHnyZObPn3/4exMmTGDHjh1s3LiRk046iWuuuYbJkydz4YUXcuBA\nroc89hDuflRNM2bM8IJcfbX7uHGFfUfkKLJ69epu3f6GDRt88uTJ7u7+9NNPe0VFha9fv/7w5zt3\n7nR397q6Op88ebLv2LHD3d3Hjx/vNTU1vmHDBi8uLva//OUv7u7+kY98xO+///4u3ouWsv1MgeWe\nZw71/Mt1ysrg0KHuroVIPG64AZ5/Pt51TpsG3/523sVnzZrV4lKXe+65h0ceCQOIb968mbVr1zJk\nSMsHMFZVVTFtWngs9owZM9i4cWPH692NFIwi0kK/fv0Ov//DH/7A7373O5YuXUpFRQXnnntu1kth\nysvLD78vLi7u8V1pBaPI0aSAll1cBgwYwN69e7N+tnv3biorK6moqODVV1/lueee6+LadQ8Fo0gv\nN2TIEM466yymTJlC3759GTFixOHPZs+ezfe+9z2mTp3KCSecwOmnn96NNe06yQjG+npwB10kK9Iu\nP81xZUd5eTmPPfZY1s9SxxGHDh3Kyy+/fHj5l770pdjr19V6/uU6ZWXhtb6+e+shIomRnGBUd1pE\nYqJgFBHJoGAUEcmgYBQRyaBgFBHJoGAUkYL0798fgC1btvDhD384a5lzzz23zccgf/vb36auru7w\n/NE0jJmCUUTaZfTo0Tz88MPt/n5mMB5Nw5gpGEV6uZtuuqnFeIzz5s3jlltu4fzzz+fUU0/llFNO\n4dFHHz3iexs3bmTKlCkAHDhwgDlz5jB16lQ+9rGPtbhX+vOf/zwzZ85k8uTJfO1rXwPCwBRbtmzh\nvPPO47zzzgOahzEDuOuuu5gyZQpTpkzh29Ftkl06vFm+w/B01VTwsGNPPukO7s88U9j3RI4S3T3s\n2MqVK/2cc845PH/SSSf5pk2bfPfu3e7uXlNT48cdd5w3NTW5u3u/fv3cveVwZXfeeadfffXV7u7+\nwgsveHFxsS9btszdm4cta2ho8Pe85z3+wgsvuHvzsGUpqfnly5f7lClTfN++fb53714/+eSTfeXK\nlQUNb6Zhx9RilATpjlHHpk+fzvbt29myZQs1NTVUVlYyatQobrzxRv74xz9SVFTEm2++ybZt2xg5\ncmTWdfzxj3/ki1/8IgBTp05l6tSphz9buHAh8+fPp6Ghga1bt7J69eoWn2d69tlnueyyyw6P8vOh\nD32IZ555hksuuaTLhjdLTjAePNi99RDpwT784Q/z8MMP89ZbbzFnzhwefPBBampqWLFiBaWlpUyY\nMKHNJ+9lewjVhg0buOOOO1i2bBmVlZVcddVVba4nNO6y66rhzZITjGoxSgJ0w6hjAMyZM4drrrmG\nHTt2sGTJEhYuXMjw4cMpLS3l6aefZtOmTa1+/5xzzuHBBx/kvPPO4+WXX+bFF18EYM+ePfTr149j\njjmGbdu28dhjj3HuuecCzcOdDR069Ih1XXXVVcydOxd355FHHuH+++/vlP3ORcEoIkyePJm9e/cy\nZswYRo0axZVXXskHPvABZs6cybRp0zjxxBNb/f7nP/95rr76aqZOncq0adOYNWsWAO9+97uZPn06\nkydPZuLEiZx11lmHv3Pttddy0UUXMWrUKJ5++unDy0899VSuuuqqw+v4zGc+w/Tp07t0VHBrrdna\nHWbOnOltXf/UwmuvwQknwAMPwJVXdl7FRDrJK6+8wkknndTd1UiUbD9TM1vh7jPz+b4u1xERyaBg\nFBHJkFcwmtlsM1tjZuvMbG6Wz+82s+ej6TUzq834fKCZvWlm342r4ocpGEUkZm2efDGzYuBe4AKg\nGlhmZovcfXWqjLvfmFb+emB6xmpuA5bEUuNMCkZJAHfPermLFC6O8yb5tBhnAevcfb27HwIWAJe2\nUv4K4KHUjJnNAEYAT3SkojkpGKWH69OnDzt37ozlF7q3c3d27txJnz59OrSefC7XGQNsTpuvBv4q\nW0EzGw9UAU9F80XAncAngfM7VNNcFIzSw40dO5bq6mpqamq6uyqJ0KdPH8aOHduhdeQTjNna97n+\ntM0BHnb3xmj+C8Bid9/cWjfBzK4FrgU49thj86hSmqIiKClRMEqPVVpaSlVVVXdXQ9LkE4zVwLi0\n+bHAlhxl5wB/nzZ/BnC2mX0B6A+Umdk+d29xAsfd5wPzIVzHmGfdm+nZ0iISo3yCcRkwycyqgDcJ\n4ffxzEJmdgJQCSxNLXP3K9M+vwqYmRmKsVAwikiM2jz54u4NwHXA48ArwEJ3X2Vmt5rZJWlFrwAW\neHccQVYwikiM8rpX2t0XA4szlt2cMT+vjXX8GPhxQbXLl4JRRGLU8+98gRCMGnZMRGKSjGAsL1eL\nUURik5xgbGPwSxGRfCUjGPv0UTCKSGySE4w6xigiMUlGMKorLSIxSkYwqistIjFKTjCqKy0iMUlG\nMKorLSIxSkYwqsUoIjFKTjCqxSgiMUlGMKorLSIxSkYwprrSGhpeRGKQnGB0h/r67q6JiCRAMoKx\nvDy8qjstIjFIRjCmngimM9MiEoNkBaNajCISg2QEo7rSIhKjZASjutIiEqNkBaNajCISg2QEY6or\nrRajiMQgGcGoFqOIxEjBKCKSIRnBqK60iMQoGcGoFqOIxEjBKCKSIRnBqK60iMQoGcGoFqOIxEjB\nKCKSIRnBWFYWXtWVFpEYJCMYzfR4AxGJTTKCEfSkQBGJTXKCUS1GEYlJXsFoZrPNbI2ZrTOzuVk+\nv9vMno+m18ysNlo+3sxWRMtXmdnn4t6Bw/QIVRGJSUlbBcysGLgXuACoBpaZ2SJ3X50q4+43ppW/\nHpgezW4FznT3g2bWH3g5+u6WOHcCUDCKSGzyaTHOAta5+3p3PwQsAC5tpfwVwEMA7n7I3VMH/srz\n3F779O0LBw502upFpPfIJ6jGAJvT5qujZUcws/FAFfBU2rJxZvZitI5vdEprERSMIhKbfILRsizL\n9WT7OcDD7t54uKD7ZnefCrwL+LSZjThiA2bXmtlyM1teU1OTT72PVFEBdXXt+66ISJp8grEaGJc2\nPxbI1eqbQ9SNzhS1FFcBZ2f5bL67z3T3mcOGDcujSlmoxSgiMcknGJcBk8ysyszKCOG3KLOQmZ0A\nVAJL05aNNbO+0ftK4CxgTRwVP4JajCISkzbPSrt7g5ldBzwOFAM/cvdVZnYrsNzdUyF5BbDA3dO7\n2ScBd5qZE7rkd7j7S/HuQkTBKCIxaTMYAdx9MbA4Y9nNGfPzsnzvSWBqB+qXP3WlRSQmybnzRS1G\nEYlJcoIx1WL0XCfMRUTyk5xgrKiAxkaor+/umohID5ecYOzbN7yqOy0iHZScYKyoCK86ASMiHZS8\nYFSLUUQ6KDnBmOpKq8UoIh2UnGBUi1FEYpKcYNTJFxGJSXKCUSdfRCQmyQlGtRhFJCbJCUa1GEUk\nJskLRrUYRaSDkhOM6kqLSEySE4zqSotITJITjGVlYKYWo4h0WHKC0Sy0GtViFJEOSk4wQjjOqBaj\niHRQsoJRo3iLSAySF4zqSotIByUrGNWVFpEYJCsY1WIUkRgkKxjVYhSRGCQrGHXyRURikKxgTD1C\nVUSkA5IVjGoxikgMkhWMajGKSAySFYxqMYpIDJIXjAcOgHt310REerBkBaMeoSoiMUhWMPbrF17V\nnRaRDkhmMO7f3731EJEeLVnB2L9/eN23r3vrISI9Wl7BaGazzWyNma0zs7lZPr/bzJ6PptfMrDZa\nPs3MlprZKjN70cw+FvcOtJBqMSoYRaQDStoqYGbFwL3ABUA1sMzMFrn76lQZd78xrfz1wPRotg74\nlLuvNbPRwAoze9zda+PcicNSLUZ1pUWkA/JpMc4C1rn7enc/BCwALm2l/BXAQwDu/pq7r43ebwG2\nA8M6VuVWqMUoIjHIJxjHAJvT5qujZUcws/FAFfBUls9mAWXA64VXM09qMYpIDPIJRsuyLNcV1HOA\nh929scUKzEYB9wNXu3vTERswu9bMlpvZ8pqamjyqlINajCISg3yCsRoYlzY/FtiSo+wcom50ipkN\nBH4L/Iu7P5ftS+4+391nuvvMYcM60NNWi1FEYpBPMC4DJplZlZmVEcJvUWYhMzsBqASWpi0rAx4B\n/svdfx5PlVuhFqOIxKDNYHT3BuA64HHgFWChu68ys1vN7JK0olcAC9xb3Kj8UeAc4Kq0y3mmxVj/\nlsrKoLRULUYR6ZA2L9cBcPfFwOKMZTdnzM/L8r0HgAc6UL/C9e+vFqOIdEiy7nyB0J1Wi1FEOiB5\nwagWo4h0UPKCsV8/BaOIdEjygrF/f3WlRaRDkheMajGKSAclLxjVYhSRDkpeMKrFKCIdlLxgVItR\nRDoomcGoFqOIdEDygrFfP2hogEOHursmItJDJS8Y9dwXEemg5AWjRtgRkQ5KXjBqTEYR6aDkBuPe\nvd1bDxHpsZIXjAMGhFcFo4i0U/KCceDA8KpgFJF2Sm4w7tnTvfUQkR4recGY6korGEWknZIXjOpK\ni0gHJS4Y/+4L5VSwnwm3f45f/7q7ayMiPVHignHZcuMAFWzaU8kll0BNTXfXSER6msQFoztcVvHf\n3D3zQQDq6rq5QiLS4yQyGItKixnatB2A+vpurpCI9DiJC8amJrCSEkoPhnulNciOiBQqccHoDlZa\nQuk74ay0WowiUqjEBmPZgd0AvPRSaEWKiOQrkcFYVFpM5cG3APjkJ+HRR7u5UiLSoyQuGJuaQovx\nzAO/51e/Cstqa7u3TiLSsyQuGN3Bykqwuv2c+u5GABobu7lSItKjJDMYS0sBKDoQBqvVMUYRKUQi\ng7GorASA4rpwZlotRhEpROKCsakpdKUBig+EaxkVjCJSiMQFYzjGGLrSxfvD0GMKRhEpRF7BaGaz\nzWyNma0zs7lZPr/bzJ6PptfMrDbts/82s1oz+02cFc+lRTCqKy0i7VDSVgEzKwbuBS4AqoFlZrbI\n3Venyrj7jWnlrwemp63iW0AF8Nm4Kt2acIxRLUYRab98WoyzgHXuvt7dDwELgEtbKX8F8FBqxt1/\nD3TZqLHhGGN0VnrfnsPLRETylU8wjgE2p81XR8uOYGbjgSrgqY5XrX3cwcrVlRaR9ssnGC3LMs9R\ndg7wsLsXFEVmdq2ZLTez5TUdHFnWHYpSwbg3HOp89FF47rkOrVZEepF8grEaGJc2PxbYkqPsHNK6\n0fly9/nuPtPdZw4bNqzQr2esC6y4GPr0oXjfbs45B5Yvh3vv7dBqRaQXyScYlwGTzKzKzMoI4bco\ns5CZnQBUAkvjrWJhmprADBg4ENu7hyVL4Ljj1J0Wkfy1GYzu3gBcBzwOvAIsdPdVZnarmV2SVvQK\nYIG7t+hmm9kzwM+B882s2szeG1/1s9U3CsZBgw6PHlFUpGAUkfy1ebkOgLsvBhZnLLs5Y35eju+e\n3d7KtYd7CEIqK2HXLgCKixWMIpK/ZN75YrQIxqIiXbIjIvlLXDAePsY4aJBajCLSLokJRne4+WbY\nvz+txRgdY1QwikghEhOMO3bAbbfBgAFw5pk0d6Xd1ZUWkYIkJhhTwXfbbfCxjxGCsbER9u1Ti1FE\nCpKYYExdJGSp+3QGDQqvu3bpch0RKUjigvGwysrwumsXxcXqSotI/hIXjIdbjKlgrK2lqAh274ad\nO7ulaiLSwyQ/GHftYsAAWLkSzjqrW6omIj1McoMx7Rjjf/wHzJ6tFqOI5CcxwZiSrcU4ZkwYSELH\nGUUkH4kJxiNajAMHhhld5C0iBUpuMBYV6bZAEWmX5AYjKBhFpF2SHYwaekxE2iHZwThkyOFT0QpG\nEclX4oKxhWHDwugSaExGEclf4oKxRYtx6FCInjpYXBzKZA1QEZE0iQnGlBbBOGwY7NkDBw9SXBwW\nqTstIm1JTDDmbDEC7Nx5OBj/5m9g794urZqI9DDJDsbUM6prarjkknCv9DPPwPr1XV49EelBekcw\n7tjBlClw001htqGhS6smIj1MsoMx1ZWOTsCURA+LVTCKSGuSHYxpLUZoDsb6+q6rl4j0PMkOxsGD\nwwK1GEWkAMkOxuLiEI4KRhEpQGKCMaVFMAKMGAHbtgFQWhoWbd/etXUSkZ4lMcGY846WMWPgzTcB\n6NcvLLrySnjwwa6pl4j0PIkLxiNajKNHw5YtAEyZAgsWhMVRVoqIHKF3BOPWrdDUhBlcfnlYfOhQ\nl1ZPRHqQ5AfjmDHhbEvaYBJmCkYRyS35wTh6dHiNutNmUFamYBSR3JIfjGPGhNe0g4rl5fCTn8D8\n+V1TNxHpWfIKRjObbWZrzGydmc3N8vndZvZ8NL1mZrVpn33azNZG06fjrHy6fFuMADfcAO+8Aw8/\n3Fm1EZGerKStAmZWDNwLXABUA8vMbJG7r06Vcfcb08pfD0yP3g8GvgbMBBxYEX13V6x7QSvBOGJE\nGL67uvrwoltugSVL4ODBuGshIkmQT4txFrDO3de7+yFgAXBpK+WvAB6K3r8XeNLd347C8Elgdkcq\n3JYjgrG0FMaOhY0bWywuL1cwikh2+QTjGGBz2nx1tOwIZjYeqAKeKvS7HZWzxQhQVXXEIIzl5bBv\nH9TVdUZtRKQnyycYs0VNrvtM5gAPu3vqAQJ5fdfMrjWz5Wa2vCa6rKZQrQbjxImwYUOLRQMGwKpV\ncMIJ7dqciCRYPsFYDYxLmx8LbMlRdg7N3ei8v+vu8919prvPHJYaKqwA27fDHXe0UmDixHDy5cCB\nw4u+/nW49NJw6FGDSohIunyCcRkwycyqzKyMEH6LMguZ2QlAJbA0bfHjwIVmVmlmlcCF0bJY/f73\n8LOfhR7ziSdmKTBxYnhNO844YQKcfXZ4n5aXIiJtB6O7NwDXEQLtFWChu68ys1vN7JK0olcAC9yb\nh3Nw97eB2wjhugy4NVoWq9ST/554At71riwFqqrCa0Z3uqIivJ51lu6dFpFmbV6uA+Dui4HFGctu\nzpifl+O7PwJ+1M765SUVxUW5Yj7VYsw4AXPxxXDJJbBoEbz0UvO14CLSuyXizpempvCa9cQLwPDh\n0L8/vPZai8Xjx8O//Vt4/4tfHB62UUSOMm+/DU89BStXwrJl8OSTsG5d520vrxbj0S4VjDlbjGYw\neTK8/PIRH40eHcZp/MEPYORIuO22zqunSG/W1BSuHT54EB5/PBzbP3QoPIPp4EF4443wzPfGxnBC\n9I03YPPmEIp79hy5vv/9v8NJ1M6QiGBs9VKdlFNOgV/9KhROK1hZGQbeGTs2NCj37IGBAzu3viI9\nSUMDrF0bAqypCXbvDofr164N4bZnT1jW2BiCrKkpjGJVVNTy9Zlnmhsx2ZSWhs5dcXGYhgyBqVPD\nKYLycjj22PC7aRZOno4bl3tdHZWIYGyzxQghGH/wg9BfHjmyxUd9+4ZFCxeGBwr+7ndthKzIUcQ9\ntLRefz0MPbprVxgLoKYmXI524EAIrfp6eOut8N49/N40NIQwO3QoLM815TJgAPTpE55UbBZeBwwI\n625sbH5tbAwnOY8/HiZNgmOOgfe+N4x0VVYWQrGiIrw/GiQiGPNqMU6ZEl5feumIYAR45BG4/vpw\nZnv8+PDXsLw8/rqKQAiM/fvDH/Py8vCgtsbGEGR1dVBbC5s2hfOFqfFPzELgvfFG+O7evSHQamtD\n4GXTt294HlyqFTZ0aFhWVBS2WV4eLlsbNKi5TLbp2GND7yr1vQkTwjnN1ONCkiYRwZh3ixFCMF5w\nwREfH388/Pu/w1e+Ekbd+cQnwnyWDBWhri4EU31983ToUJgaGpqPk23ZEi6f3bw5hBKEP75//nNz\nS6y4OATMvn3Zu5r9+4cAcw9TVVUIsiFDQmsNwv/fd70rBNiYMaH8kCEhBIuLu+RHkii9JxiHDQtn\nWlasyFnk+OPDWeq1a0M4jh4N//Iv4auSHI2NoZVVURGCxSyE2Ouvh7OeGzfCzp3NJwZSrbKamhBw\nBw6Eu61yPoAti/RBS4qK4LTTwt/qSZNCa6+pKdRn/PjQzezfP4TflClhXod2ulYigjGvrjTA6afD\nc8+1WmTSJFi6NPx1v+ee0JW54w7dU90TNDWFENu0CV59NbTqnn02BM+aNSGY9u4NZznr68N3Ul3F\nzBHdKypCmJWWhuNeffuG/xOnnx5OAAweHA7+l5a2nFLd4tRUURG6nSNHhkAtK2t+vrkcvRLxT5RX\nixHgjDPgl78Mf+6HD89ZrG/fcNbt8svhN78J0333wec+p7/ccWtsDAFWVBSmmppwAqyhIYTXhg3N\nx9L27YNXXgkBs3dvKLtvX2jFtTaEnBmcey6MGhWCa/DgMExnQ0NYT1NTWD56NBx3XLiya9So+Pc1\ndaeVHP0SEYx5txjPOCO8Ll0aRpBoxfDh8Nvfwle/GlqOX/hCmC67LBx/THWDJLTAdu4MgbVlSzg5\nUF0dWmqp4Nq3r+UJg/SpEIMHhwDr0ydcYtW/P7zvfaEVl2r9jRwZWnMTJoRW3IQJefzRFEmTiGDM\nu8U4Y0b4TfnTn9oMRgi/bN/5Dtx+O9x1Vzje+MgjYYJw7OeUU8K1VhdcAO95TzhzlxS7d4cwq6sL\n95KvWxeWrV8frknbsCG01HKF28iR4aRCKqj69QuXcpSXN1+mUVYWlqUuHykrC2c7S0tDl3PIkBCA\nqS5tRYVCTjqfeSFHkLvAzJkzffny5QV95+674R//MVy/NWhQG4XPOy80cV54oeC61deHltEzz4RD\nlS++GK55TDdoEPz1X8OZZzZfs3XyyZ17XCkVKqnp4MHmM5zZrklragqtuE2bQsvu4MHQ0tu9O1zm\nuWlT2Nfdu7Nvr7g47NvEieHkwIAB4WxoeXnoog4fHs6M6kJ5OZqY2Qp3n5lP2US0GNscRCLdxRfD\nl78cmkAFjhpRWhpaPx/5SJgdYfk+AAANGUlEQVQgHKdaty5cfvGXv4Sba559NhyXTFdeHrp9gweH\nwCgqCl3/1LG1Pn1CqDQ2hmNnqQBLTanwq6lpPouZKtPRv20VFaFlNmJEqNsHPxiWDR4cjrWVlITX\nY49tbvnpEhBJskQEY5uDSKRLBeNvfgOf/WyHt11SEsaAPPFE+NSnQuu1qSm0vF57DVavDicT9u0L\nx9e2bm2+tSoz8N54I6xv0KAQlCUlRwbo6NHwoQ+FbmXqdqvMW69KSkIrLjWfORUVhZCeODFc+6au\nqUhLiQjGglqMJ58crr156KFYgjGboqLQwho1Khx3FJGeJRFthYJajGbhtPKSJeFgmohIhkQFY95d\nwk98Irzef3+n1EdEerYeH4y33grf/W54n/fF1xMmhOtr7rtPD5cWkSP0+GAcOBCmT4cbbihwyKIv\nfzmcCXnggU6rm4j0TIm4jrFd3MOd/Dt2hPvM+vbt/G2KSLcp5DrGHt9ibDcz+Na3wgmYO+/s7tqI\nyFGk9wYjhLtgLr88PDgiy/NgRKR36t3BCHDvveFA5RVXhOGRRaTXUzCOGAE//nFoMX7hCx2/v05E\nejwFI8BFF8HNN8N//ifcckt310ZEulkibgmMxbx54WblW24JAwvefrtGpRXppRSMKWbh8ap9+8I3\nvxke/PLDHyZrgEURyYu60umKi8PJmLvugl//Olw5/tvf6rijSC+jYMxkBjfeGEb5Li+H978fzj8/\nDDqhgBTpFRSMucyaFZ5Bfc89sGpVeJrS2WeH56rqsh6RRFMwtqasDK6/Pjxo+LvfDc8D+MhHwjDe\nV10FDz4Y7rcWkUTpvfdKt0djIzz1VBiu7De/CQ+ZgTDm/2mnwcyZ4elYJ54YRvDR+P8iR41e98yX\nLlNcHIYru+CCEJLPPw9/+AMsWwbLl8MvftFctqwsPAnrhBOgqioE5fjxzdMxx3TXXohIG/IKRjOb\nDXwHKAZ+4O63ZynzUWAe4MAL7v7xaPk3gPdFxW5z95/FUO/uV1wcHsc6Y0bzsl27wkg9a9bAq6+G\nadUqWLz4yOOSgwY1h2R6aKbeDxmi6yhFukmbwWhmxcC9wAVANbDMzBa5++q0MpOAfwbOcvddZjY8\nWv4+4FRgGlAOLDGzx9x9T/y7chSorAzPTT3zzJbL3WH79jCSz6ZN4Zhl6v369fD00+FJWekqKo5s\nZY4fH55sOHJkmAYMUHiKdIJ8WoyzgHXuvh7AzBYAlwKr08pcA9zr7rsA3H17tPxkYIm7NwANZvYC\nMBtYGFP9ewazcE/2iBHhbHcmd6itPTI0U/N//nN4oHWmiormkBw5Mjx9K9v88OHh2a8ikpd8gnEM\nsDltvhr4q4wyxwOY2Z8I3e157v7fwAvA18zsLqACOI+WgSoQgrOyMkzTpmUvs29fuGVx69bwYOnU\na+r9q6+G451vv519/UOHHhmaI0bAsGHN09Ch4bWiolN3V+Rol08wZuurZZ7KLgEmAecCY4FnzGyK\nuz9hZqcB/w+oAZYCDUdswOxa4FqAY489Nu/K9yr9+4dHv558cuvlDh4MD7XOFp6p92vWhNdDh7Kv\no6Iie2BmTqnlxxyjLr0kSj7BWA2MS5sfC2zJUuY5d68HNpjZGkJQLnP3rwNfBzCznwJrMzfg7vOB\n+RAu1yl0JyRNeXm4fKitPzCp7vuOHVBT03JKX7Z9eziBVFMTBtfIprS0ZXi2FaRDhuhSJjmq5ROM\ny4BJZlYFvAnMAT6eUeZXwBXAj81sKKFrvT46cTPI3Xea2VRgKvBEbLWX9kvvvk+alN936upaD9HU\ntGJFWF5bm3vbgweHkBw8OLQ4Bw068jXbsmOOCS1atVClE7UZjO7eYGbXAY8Tjh/+yN1XmdmtwHJ3\nXxR9dqGZrQYagX+KwrAPoVsNsAf4RHQiRnqiiorms+P5qK9vGZzZQnTXrvC6di3s3h3CtL6+9fWW\nlLQMylTADx4cpmzvU2X691eoSpt054scXdxDlz0Vkm29pqa33w4h+/bbrQdrSUlzSFZWNrdMM1uq\nuaY+fRSsPZTufJGeyyy0TCsqwpnzQrnD/v3NIZkemJmvtbXh/YYNze/baq2WlrYMysxwTZ8fOrT5\nuOrQoeH4r/QICkZJFrPQXe7fH8aNa7t8Ovdwh1J6SzRb6zQVoqnlmzY1L8t1ph9CnVIhmR6Y2ZYN\nHRpCtkjjvHQHBaNIilkYwb1v3/a3VlPBumtXuCg/dWw1dXw19X77dli9Oiyrq8u+vqKicAY/FZaV\nlaEl3bdv82vm+3w/Ky/XIYFWKBhF4tLeYK2raz1EU+/Xrw/HX1NTXV37xwY1C8dL8w3YzPl+/Zpb\n5unvBwwIjyMeOLBH322lYBTpbqljqoV2/aG5lZoKyszgzDXfVtlt27J/1tYx2HR9+oSgrKgI79On\n8vLwWlYW3peVNU+lpWFKfZZrOvFEOOmkwn9meVAwivRk6a3UwYM7f3sNDc1huW9fmPbvD69794Zp\nz54w7d0bjsO+807LKXXVwbZt4Zhs+nTwYAjf+vrWj9cCfOUr8K//2im7qWAUkfyVlIRW4IAB4V77\nzuQeAvLgwezTsGGdtmkFo4gcncyau9cDBnTppnUtgIhIBgWjiEgGBaOISAYFo4hIBgWjiEgGBaOI\nSAYFo4hIBgWjiEgGBaOISAYFo4hIhqPu0QZmVgNsKvBrQ4EdnVAdbf/o3nZv335v3vf2bH+8u+d1\ng/VRF4ztYWbL832Wg7afnG339u335n3v7O2rKy0ikkHBKCKSISnBOF/b75Xb7u3b78373qnbT8Qx\nRhGROCWlxSgiEpseH4xmNtvM1pjZOjOb2wnrH2dmT5vZK2a2ysz+IVo+z8zeNLPno+nitO/8c1Sf\nNWb23hjqsNHMXoq2szxaNtjMnjSztdFrZbTczOyeaPsvmtmpHdz2CWn7+LyZ7TGzGzpz/83sR2a2\n3cxeTltW8P6a2aej8mvN7NMd2Pa3zOzVaP2PmNmgaPkEMzuQ9jP4Xtp3ZkT/Zuui+uX1rNIc2y/4\nZ93e34sc2/9Z2rY3mtnznbH/rfyudcm/fQvu3mMnoBh4HZgIlAEvACfHvI1RwKnR+wHAa8DJwDzg\nS1nKnxzVoxyoiupX3ME6bASGZiz7JjA3ej8X+Eb0/mLgMcCA04H/ifnn/RYwvjP3HzgHOBV4ub37\nCwwG1kevldH7ynZu+0KgJHr/jbRtT0gvl7GePwNnRPV6DLioA/te0M+6I78X2baf8fmdwM2dsf+t\n/K51yb99+tTTW4yzgHXuvt7dDwELgEvj3IC7b3X3ldH7vcArwJhWvnIpsMDdD7r7BmBdVM+4XQr8\nJHr/E+CDacv/y4PngEFm1o6nx2d1PvC6u7d2AX6H99/d/wi8nWW9hezve4En3f1td98FPAnMbs+2\n3f0Jd2+IZp8Dxra2jmj7A919qYff1P9Kq2/B229Frp91u38vWtt+1Or7KPBQa+to7/638rvWJf/2\n6Xp6MI4BNqfNV9N6aHWImU0ApgP/Ey26LmrC/yjVvO+kOjnwhJmtMLNro2Uj3H0rhP9QwPBO3H7K\nHFr+UnTV/kPh+9tZ9fhfhFZKSpWZ/cXMlpjZ2Wl1qo5524X8rDtr388Gtrn72rRlnbL/Gb9rXf5v\n39ODMdtxi045zW5m/YFfADe4+x7g/wLHAdOArYQuRmfV6Sx3PxW4CPh7Mzuntap2wvYxszLgEuDn\n0aKu3P9Wq5Zje7HXw8y+AjQAD0aLtgLHuvt04B+Bn5rZwE7YdqE/6876N7iCln8YO2X/s/yu5Sya\nYzsd3v+eHozVwLi0+bHAlrg3YmalhH+oB939lwDuvs3dG929Cfg+zd3F2Ovk7lui1+3AI9G2tqW6\nyNHr9s7afuQiYKW7b4vq0mX7Hyl0f2OtR3QA//3AlVH3kKgLuzN6v4JwXO/4aNvp3e0ObbsdP+vY\n/w3MrAT4EPCztHrFvv/Zftfohn/7nh6My4BJZlYVtWjmAIvi3EB0XOWHwCvuflfa8vTjdpcBqbN4\ni4A5ZlZuZlXAJMKB6PZuv5+ZDUi9J5wIeDnaTups26eBR9O2/6nojN3pwO5UN6SDWrQWumr/0xS6\nv48DF5pZZdT1vDBaVjAzmw3cBFzi7nVpy4eZWXH0fiJhX9dH299rZqdH/38+lVbf9my/0J91Z/xe\n/C3wqrsf7iLHvf+5ftfojn/7Qs7UHI0T4czUa4S/Vl/phPX/NaEZ/iLwfDRdDNwPvBQtXwSMSvvO\nV6L6rCHPs5GtbH8i4aziC8Cq1D4CQ4DfA2uj18HRcgPujbb/EjAzhp9BBbATOCZtWaftPyGAtwL1\nhL/+f9ee/SUcD1wXTVd3YNvrCMesUv/+34vKXh79m7wArAQ+kLaemYQAex34LtHNFO3cfsE/6/b+\nXmTbfrT8x8DnMsrGuv/k/l3rkn/79El3voiIZOjpXWkRkdgpGEVEMigYRUQyKBhFRDIoGEVEMigY\nRUQyKBhFRDIoGEVEMvx/NWoT3j1eTEkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3700623cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(train_loss, 'r',label='train')\n",
    "plt.plot(val_loss, 'b', label='validation')\n",
    "plt.title('Loss during training')\n",
    "plt.legend()#loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. `TFFrozenModel` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training&validation are done we saved our model. We can use the saved model for inference. Create TFFrozenModel instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from train/model\n"
     ]
    }
   ],
   "source": [
    "#reset the default graph to avoid the errors\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#create `TFFrozenModel` instance\n",
    "frozen_model = TFFrozenModel(unet, logits2pred=softmax, restore_model_path='train')\n",
    "\n",
    "img = generate_image(shape)\n",
    "y_pred = frozen_model.do_inf_step([img])\n",
    "\n",
    "#close the session\n",
    "frozen_model.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  (1, 2, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print('Output shape: ', y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAD0CAYAAAB+bCt+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f374c345828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3*4,2*4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img[0,...], cmap='gray')\n",
    "plt.title('Fake input')\n",
    "plt.subplot(132)\n",
    "plt.title('Output: 1-th modality')\n",
    "plt.imshow(y_pred[0, 0, ...], cmap='gray')\n",
    "plt.subplot(133)\n",
    "plt.title('Output: 2-th modality')\n",
    "plt.imshow(y_pred[0, 1, ...], cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
